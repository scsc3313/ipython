{
 "metadata": {
  "name": "",
  "signature": "sha256:a8bb12496028f033e4b23fe7e6d8f6db4d2e8a4278fe2357f88e0309b5f240dd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os \n",
      "os.chdir(\"/home/python/notebook_root/build_machine_learning_system_with_python/data/ch05\") \n",
      "!ls -al "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ud569\uacc4 64\r\n",
        "drwxr-xr-x  2 root root 4096 12\uc6d4  1 20:47 .\r\n",
        "drwxr-xr-x 15 root root 4096 12\uc6d4  1 20:56 ..\r\n",
        "-rw-r--r--  1 root root 9486 12\uc6d4  1 20:47 PosTagFreqVectorizer.py\r\n",
        "-rw-r--r--  1 root root  798 12\uc6d4  1 20:47 README.md\r\n",
        "-rw-r--r--  1 root root 7659 12\uc6d4  1 20:47 chose_instances.py\r\n",
        "-rw-r--r--  1 root root 8098 12\uc6d4  1 20:47 classify.py\r\n",
        "-rw-r--r--  1 root root  530 12\uc6d4  1 20:47 data.py\r\n",
        "-rw-r--r--  1 root root 3203 12\uc6d4  1 20:47 log_reg_example.py\r\n",
        "-rw-r--r--  1 root root 5084 12\uc6d4  1 20:47 so_xml_to_tsv.py\r\n",
        "-rw-r--r--  1 root root 7111 12\uc6d4  1 20:47 utils.py\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load so_xml_to_tsv.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "#\n",
      "# This script filters the posts and keeps those posts that are or belong\n",
      "# to a question that has been asked in 2011 or 2012.\n",
      "#\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import re\n",
      "try:\n",
      "    import ujson as json  # UltraJSON if available\n",
      "except:\n",
      "    import json\n",
      "from dateutil import parser as dateparser\n",
      "\n",
      "from operator import itemgetter\n",
      "from xml.etree import cElementTree as etree\n",
      "from collections import defaultdict\n",
      "\n",
      "from data import DATA_DIR\n",
      "\n",
      "#filename = os.path.join(DATA_DIR, \"posts-2011-12.xml\")\n",
      "filename = os.path.join(DATA_DIR, \"posts-2012.xml\")\n",
      "print(\"Reading from xml %s\" % filename)\n",
      "filename_filtered = os.path.join(DATA_DIR, \"filtered.tsv\")\n",
      "print(\"Filtered: %s\" % filename_filtered)\n",
      "filename_filtered_meta = os.path.join(DATA_DIR, \"filtered-meta.json\")\n",
      "print(\"Meta: %s\" % filename_filtered_meta)\n",
      "\n",
      "q_creation = {}  # creation datetimes of questions\n",
      "q_accepted = {}  # id of accepted answer\n",
      "\n",
      "# question -> [(answer Id, IsAccepted, TimeToAnswer, Score), ...]\n",
      "meta = defaultdict(list)\n",
      "\n",
      "# regegx to find code snippets\n",
      "code_match = re.compile('<pre>(.*?)</pre>', re.MULTILINE | re.DOTALL)\n",
      "link_match = re.compile(\n",
      "    '<a href=\"http://.*?\".*?>(.*?)</a>', re.MULTILINE | re.DOTALL)\n",
      "img_match = re.compile('<img(.*?)/>', re.MULTILINE | re.DOTALL)\n",
      "tag_match = re.compile('<[^>]*>', re.MULTILINE | re.DOTALL)\n",
      "\n",
      "\n",
      "def filter_html(s):\n",
      "    num_code_lines = 0\n",
      "    link_count_in_code = 0\n",
      "    code_free_s = s\n",
      "\n",
      "    num_images = len(img_match.findall(s))\n",
      "\n",
      "    # remove source code and count how many lines\n",
      "    for match_str in code_match.findall(s):\n",
      "        num_code_lines += match_str.count('\\n')\n",
      "        code_free_s = code_match.sub(\"\", code_free_s)\n",
      "\n",
      "        # sometimes source code contain links, which we don't want to count\n",
      "        link_count_in_code += len(link_match.findall(match_str))\n",
      "\n",
      "    links = link_match.findall(s)\n",
      "    link_count = len(links)\n",
      "\n",
      "    link_count -= link_count_in_code\n",
      "\n",
      "    link_free_s = re.sub(\n",
      "        \" +\", \" \", tag_match.sub('', code_free_s)).replace(\"\\n\", \"\")\n",
      "\n",
      "    for link in links:\n",
      "        if link.lower().startswith(\"http://\"):\n",
      "            link_free_s = link_free_s.replace(link, '')\n",
      "\n",
      "    num_text_tokens = link_free_s.count(\" \")\n",
      "\n",
      "    return link_free_s, num_text_tokens, num_code_lines, link_count, num_images\n",
      "\n",
      "years = defaultdict(int)\n",
      "num_questions = 0\n",
      "num_answers = 0\n",
      "\n",
      "if sys.version_info.major < 3:\n",
      "    # Python 2, map() returns a list, which will lead to out of memory errors.\n",
      "    # The following import ensures that the script behaves like being executed\n",
      "    # with Python 3.\n",
      "    from itertools import imap as map\n",
      "\n",
      "\n",
      "def parsexml(filename):\n",
      "    global num_questions, num_answers\n",
      "\n",
      "    counter = 0\n",
      "\n",
      "    it = map(itemgetter(1),\n",
      "             iter(etree.iterparse(filename, events=('start',))))\n",
      "\n",
      "    root = next(it)  # get posts element\n",
      "\n",
      "    for elem in it:\n",
      "        if counter % 100000 == 0:\n",
      "            print(\"Processed %i <row/> elements\" % counter)\n",
      "\n",
      "        counter += 1\n",
      "\n",
      "        if elem.tag == 'row':\n",
      "            creation_date = dateparser.parse(elem.get('CreationDate'))\n",
      "\n",
      "            Id = int(elem.get('Id'))\n",
      "            PostTypeId = int(elem.get('PostTypeId'))\n",
      "            Score = int(elem.get('Score'))\n",
      "\n",
      "            if PostTypeId == 1:\n",
      "                num_questions += 1\n",
      "                years[creation_date.year] += 1\n",
      "\n",
      "                ParentId = -1\n",
      "                TimeToAnswer = 0\n",
      "                q_creation[Id] = creation_date\n",
      "                accepted = elem.get('AcceptedAnswerId')\n",
      "                if accepted:\n",
      "                    q_accepted[Id] = int(accepted)\n",
      "                IsAccepted = 0\n",
      "\n",
      "            elif PostTypeId == 2:\n",
      "                num_answers += 1\n",
      "\n",
      "                ParentId = int(elem.get('ParentId'))\n",
      "                if not ParentId in q_creation:\n",
      "                    # question was too far in the past\n",
      "                    continue\n",
      "\n",
      "                TimeToAnswer = (creation_date - q_creation[ParentId]).seconds\n",
      "\n",
      "                if ParentId in q_accepted:\n",
      "                    IsAccepted = int(q_accepted[ParentId] == Id)\n",
      "                else:\n",
      "                    IsAccepted = 0\n",
      "\n",
      "                meta[ParentId].append((Id, IsAccepted, TimeToAnswer, Score))\n",
      "\n",
      "            else:\n",
      "                continue\n",
      "\n",
      "            Text, NumTextTokens, NumCodeLines, LinkCount, NumImages = filter_html(\n",
      "                elem.get('Body'))\n",
      "\n",
      "            values = (Id, ParentId,\n",
      "                      IsAccepted,\n",
      "                      TimeToAnswer, Score,\n",
      "                      Text.encode(\"utf-8\"),\n",
      "                      NumTextTokens, NumCodeLines, LinkCount, NumImages)\n",
      "\n",
      "            yield values\n",
      "\n",
      "            root.clear()  # preserve memory\n",
      "\n",
      "with open(filename_filtered, \"w\") as f:\n",
      "    for values in parsexml(filename):\n",
      "        line = \"\\t\".join(map(str, values))\n",
      "        f.write(line + \"\\n\")\n",
      "\n",
      "with open(filename_filtered_meta, \"w\") as f:\n",
      "    json.dump(meta, f)\n",
      "\n",
      "print(\"years:\", years)\n",
      "print(\"#qestions: %i\" % num_questions)\n",
      "print(\"#answers: %i\" % num_answers)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading from xml data/posts-2012.xml\n",
        "Filtered: data/filtered.tsv\n",
        "Meta: data/filtered-meta.json\n"
       ]
      },
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'data/filtered.tsv'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-69278fec9d17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# preserve memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_filtered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparsexml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/filtered.tsv'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}