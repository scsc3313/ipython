{
 "metadata": {
  "name": "",
  "signature": "sha256:d2c88f1160a40a1ab5ae7addced888e880b9e727ffa044ef550f5c284ef55ed6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os \n",
      "os.chdir(\"/home/python/notebook_root/build_machine_learning_system_with_python/data/ch03\") "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load rel_post_01.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "from utils import DATA_DIR\n",
      "\n",
      "TOY_DIR = os.path.join(DATA_DIR, \"toy\")\n",
      "posts = [open(os.path.join(TOY_DIR, f)).read() for f in os.listdir(TOY_DIR)]\n",
      "\n",
      "new_post = \"imaging databases\"\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "\n",
      "class StemmedCountVectorizer(CountVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "# vectorizer = CountVectorizer(min_df=1, stop_words='english',\n",
      "# preprocessor=stemmer)\n",
      "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(\n",
      "    min_df=1, stop_words='english', decode_error='ignore')\n",
      "\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "print(new_post_vec, type(new_post_vec))\n",
      "print(new_post_vec.toarray())\n",
      "print(vectorizer.get_feature_names())\n",
      "\n",
      "\n",
      "def dist_raw(v1, v2):\n",
      "    delta = v1 - v2\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "\n",
      "    delta = v1_normalized - v2_normalized\n",
      "\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "dist = dist_norm\n",
      "\n",
      "best_dist = sys.maxsize\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
      "print(\"source : %s\" % new_post)\n",
      "print(\"most simmrarity : %s\"%posts[best_i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load rel_post_20news.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}