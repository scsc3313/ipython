{
 "metadata": {
  "name": "",
  "signature": "sha256:aa265a03c8e2ab0bbd24193b85aa8dbff2f08465fdd09d053ff86f49298befb0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "3. \uad70\uc9d1\ud654 : \uad00\ub828\ub41c \uac8c\uc2dc\ubb3c \ucc3e\uae30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2\uc7a5 \uc8fc\uc81c\n",
      "    - \uc9c0\ub3c4 \ud559\uc2b5(supervised learning)-\ubd84\ub958(Classification) : \ud6c8\ub828 \ub370\uc774\ud130(Training Data)\ub85c \ud558\ub098\uc758 \ud568\uc218\ub97c \uc720\ucd94\ud574\uc11c \uc0c8\ub85c\uc6b4 \uc785\ub825\uc774 \uc5b4\ub5a4 \uc885\ub958\uc778\uc9c0 \ubd84\ub958 \n",
      "\n",
      "3\uc7a5 \uc8fc\uc81c\n",
      "    - \uad70\uc9d1\ud654(Clustering): \ub370\uc774\ud130 \uc790\uccb4\uc5d0\uc11c \ud328\ud134\uc744 \ud30c\uc545\ud558\uc5ec \uad6c\uccb4\uc801\uc778 \ud2b9\uc131\uc744 \uacf5\uc720\ud558\ub294 \uad70\uc9d1\uc744 \ucc3e\uc74c (\uad70\uc9d1\ud654\ub294 \ubbf8\ub9ac \uc815\uc758\ub41c \ud2b9\uc131\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uac00\uc9c0\uc9c0 \uc54a\uc74c)\n",
      "\n",
      "3\uc7a5\uc758 \ub0b4\uc6a9 \n",
      "    - \ud14d\uc2a4\ud2b8 \uac8c\uc2dc\ubb3c\uc5d0 \uad70\uc9d1\ud654\ub97c \uc801\uc6a9\ud558\uc5ec \uc785\ub825\ub41c \uc0c8 \uac8c\uc2dc\ubb3c\uacfc \uad00\ub828\ub41c \ubaa8\ub4e0 \uac8c\uc2dc\ubb3c\uc744 \ube60\ub974\uac8c \ucc3e\ub294 \ubc29\ubc95\uc744 \uc2e4\uc2b5\n",
      "    - \uad70\uc9d1\ud654\uc5d0 Scikit-learn\uc744 \uc774\uc6a9\ud558\ub294 \ubc29\ubc95\n",
      "\n",
      "3\uc7a5\uc758 \uad6c\uc131 \n",
      "    - Part 1 : \uc804\ucc98\ub9ac \uacfc\uc815 - \uc6d0\uc2dc \ud14d\uc2a4\ud2b8\ub97c \uc758\ubbf8\uc788\ub294 \uc218\uce58\ub85c \ubcc0\ud658 (\ubca1\ud130\ud654 : \ub2e8\uc5b4 \ube48\ub3c4\uc218 > TF-IDF \uac12) \n",
      "    - Part 2 : \uad70\uc9d1\ud654 \uc801\uc6a9 - \uc2e4\uc81c \uac8c\uc2dc\ubb3c \ub370\uc774\ud130\uc5d0 \uc801\uc6a9 (\uc218\ud3c9 \uad70\uc9d1\ud654 - K \ud3c9\uade0 \uc54c\uace0\ub9ac\uc998)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\uac8c\uc2dc\ubb3c\uc758 \uad00\ub828\ub3c4 \uce21\uc815"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc720\uc0ac\ub3c4 \uce21\uc815 \ubc29\ubc95\n",
      "    1. \ub808\ubca4\uc2dc\ud0c0\uc778 \uac70\ub9ac(Levenshtein distance) \ubc29\ubc95\n",
      "    2. \ub2e8\uc5b4 \uc8fc\uba38\ub2c8(bag-of-words) \ubc29\ubc95"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\ud558\uc9c0 \ub9d0\uc544\uc57c \ud558\ub294 \ubc29\ubc95"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ub808\ubca4\uc2dc\ud0c0\uc778 \uac70\ub9ac(Levenshtein distance) \ubc29\ubc95\n",
      "    - \uc758\ubbf8 : \ub450 \ubb38\uc790\uc5f4\uc758 \ube44\uc2b7\ud55c \uc815\ub3c4\ub97c \uce21\uc815\ud558\ub294 \ubc29\ubc95 \n",
      "    - \uac70\ub9ac \uac12 : \ubb38\uc790\uc5f4\uc744 \uc77c\uce58\uc2dc\ud0a4\uae30 \uc704\ud574 \uc0ad\uc81c, \ucd94\uac00, \ub610\ub294 \uc218\uc815\uc774 \ud544\uc694\ud55c \ud69f\uc218  (machine, mchiene = 2)\n",
      "    - \ub2e8\uc810\n",
      "        1. \uc54c\uace0\ub9ac\uc998 \uc18d\ub3c4\uac00 \ub290\ub9bc - \uac01 \ubb38\uc790\uc5f4\uc744 \ubaa8\ub450 \ube44\uad50 (\uc2dc\uac04 \ubcf5\uc7a1\uc131 [Big-O])\n",
      "        2. \uc54c\uace0\ub9ac\uc998\uc774 \uacac\uace0\ud558\uc9c0 \uc54a\uc74c - \ub2e8\uc5b4, \ubb38\uc790 \uc7ac\ubc30\uce58\uac00 \uace0\ub824 \uc548\ub428"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\uc5b4\ub5bb\uac8c \ud574\uc57c \ud558\ub294\uac00"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ub2e8\uc5b4 \uc8fc\uba38\ub2c8(bag-of-words) \uc811\uadfc\ubc95\n",
      "    - \uc758\ubbf8 : \ub2e8\uc5b4 \ube48\ub3c4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubca1\ud130\ub85c \ub098\ud0c0\ub0c4 (\ub2e8\uc5b4\ub294 \uae00\ub9c8\ub2e4 \ub2e4\ub97c \uc218 \uc788\uc73c\ubbc0\ub85c \ud76c\uc18c \ud589\ub82c\uc744 \uc0ac\uc6a9\ud558\uba74 \uc88b\uc74c)\n",
      "    - \uc720\uc0ac\ub3c4 \uce21\uc815 : \uac01 \uac8c\uc2dc\ubb3c \ubca1\ud130 \uc0ac\uc774\uc5d0 \uc720\ud074\ub9ac\ub4dc \uac70\ub9ac\ub97c \uacc4\uc0b0\ud558\uc5ec \uac00\uc7a5 \uac00\uae4c\uc6b4 \uac8c\uc2dc\ubb3c\uc744 \ucc3e\uc74c\n",
      "    - \uc18d\ub3c4 \ud5a5\uc0c1 \ubc29\ubc95 (\ub290\ub9b0 \uc720\ud074\ub9ac\ub4dc \uac70\ub9ac \uacc4\uc0b0 \uc2dc\uac04\uc744 \uadf9\ubcf5) \n",
      "        1. \uac01 \uac8c\uc2dc\ubb3c\uc5d0\uc11c \ud575\uc2ec \uc18d\uc131 \ucd94\ucd9c \ud6c4 \ubca1\ud130\ud654\ud574\uc11c \uc800\uc7a5\n",
      "        2. \ud574\ub2f9 \ubca1\ud130\ub85c \uad70\uc9d1\ud654 \uacc4\uc0b0\n",
      "        3. \uc785\ub825 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \uad70\uc9d1 \uacb0\uc815\n",
      "        4. \ub2e4\uc591\uc131 \uc99d\uac00\ub97c \uc704\ud574 \uc720\uc0ac\ub3c4\uac00 \ucc28\uc774\ub098\ub294 \uc77c\ubd80 \uac8c\uc2dc\ubb3c\ub3c4 \ud65c\uc6a9\n",
      "    - \uc7a5\uc810 : \ube60\ub974\uace0, \uacac\uace0   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\uc804\ucc98\ub9ac : \uacf5\ud1b5 \ub2e8\uc5b4\uc758 \uc720\uc0ac\ud55c \uac1c\uc218\ub85c\uc11c \uce21\uc815\ub41c \uc720\uc0ac\ub3c4"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\uc6d0\uc2dc \ud14d\uc2a4\ud2b8\ub97c \ub2e8\uc5b4 \uc8fc\uba38\ub2c8\ub85c \ubcc0\ud658"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ub2e8\uc5b4\ub97c \uc138\uace0 \ubca1\ud130\ud654 \ud558\uae30 \uc704\ud574\uc11c SciKit \ub77c\uc774\ube0c\ub7ec\ub9ac\ub294 CountVectorizer\ub77c\ub294 \ud3b8\ub9ac\ud55c \ud568\uc218\ub97c \uc81c\uacf5 - \uace0\uc0dd\ud560 \ud544\uc694 \uc5c6\uc74c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "min_df - \ucd5c\uc18c \ube48\ub3c4\uc218 \uc124\uc815, \uc815\uc218 - \ud574\ub2f9 \uac12\ubcf4\ub2e4 \uc791\uc73c\uba74 \uc774\uc6a9\ud558\uc9c0 \uc54a\uc74c,\n",
      "\n",
      "CountVectorizer\uc758 \uae30\ubcf8 \ub9e4\uac1c \ubcc0\uc218 \ud655\uc778\ubc95"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(vectorizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "analyzer=word - \ub2e8\uc5b4\uc758 \uc218\ub97c \uc148,\n",
      "\n",
      "token_pattern - \ub2e8\uc5b4 \uacb0\uc815\uc740 \uc815\uaddc \ud45c\ud604\uc2dd(regular expression)\uc744 \ub530\ub984"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content=[\"How to format my hard disk\", \"Hard disk format problems\"]\n",
      "X = vectorizer.fit_transform(content)\n",
      "vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ub450 \uac1c\uc758 \ubb38\uc7a5\uc5d0\uc11c 7\uac1c\uc758 \ub2e8\uc5b4\ub97c \ucc3e\uc74c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X.toarray().transpose())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 1]\n",
        " [1 1]\n",
        " [1 1]\n",
        " [1 0]\n",
        " [1 0]\n",
        " [0 1]\n",
        " [1 0]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ub2e8\uc5b4 \ube48\ub3c4\ub97c \uc774\uc6a9\ud55c \ubca1\ud130\ud654 \uacb0\uacfc"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\ub2e8\uc5b4 \uc138\uae30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"data/toy\" \ud3f4\ub354\uc5d0 \uc788\ub294 \ub2e4\uc12f\uac1c\uc758 \uac8c\uc2dc\ubb3c\uc744 \uc774\uc6a9\ud55c \uac04\ub2e8\ud55c \uc608\uc81c \uc2e4\uc2b5 - \uc784\uc758\uc758 \uac8c\uc2dc\ubb3c\uacfc \uac00\uc7a5 \uc791\uc740 \uac70\ub9ac \uac12\uc744 \uac00\uc9c4 \uac8c\uc2dc\ubb3c \ucc3e\uae30"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir('/backup/dev/ipython/build_machine_learning_system_with_python/data/ch03/')\n",
      "posts = [open(os.path.join(\"data/toy\", f)).read() for f in os.listdir(\"data/toy\")]\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1)\n",
      "\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 25\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "5\uac1c\uc758 \uac8c\uc2dc\ubb3c, 25\uac1c\uc758 \ud2b9\uc9d5 (\ub2e8\uc5b4 \uc218)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'this', u'toy']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc0c8\ub85c\uc6b4 \uac8c\uc2dc\ubb3c\uc758 \ubca1\ud130\ud654 \uacfc\uc815 - 'imaging databases'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = \"imaging databases\"\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "\n",
      "print(new_post_vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "transform\uc5d0\uc11c \ubc18\ud658\ub418\ub294 \ubca1\ud130\ub294 \ud76c\uc18c(sparse) \ubc30\uc5f4(\ud589\ub82c) \uc774\ub2e4. - \uc0c8\ub85c\uc6b4 \uac8c\uc2dc\ubb3c\uc740 \ub2e8\uc5b4 \uc218\uac00 \uc801\uc73c\ubbc0\ub85c \ub300\ubd80\ubd84\uc774 0\uc778 \uac12\uc744 \uac00\uc9c8 \uac83\uc774\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(new_post_vec.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "toarray \uba54\uc18c\ub4dc\ub85c \uc804\uccb4 \ubc30\uc5f4\uc5d0 \uc811\uadfc \uac00\ub2a5\n",
      "\n",
      "\uc720\uc0ac\ub3c4 \uacc4\uc0b0\uc5d0\ub294 \uc804\uccb4 \ubc30\uc5f4\uc744 \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. \n",
      "\n",
      "\uc720\uc0ac\ub3c4 \uacc4\uc0b0\uc744 \uc704\ud574 \uc0c8\ub85c\uc6b4 \uac8c\uc2dc\ubb3c\uc758 \uce74\uc6b4\ud2b8 \ubca1\ud130\uc640 \ubaa8\ub4e0 \uac8c\uc2dc\ubb3c \uce74\uc6b4\ud2b8 \ubca1\ud130 \uc0ac\uc774\uc758 \uc720\ud074\ub9ac\ub4dc \uac70\ub9ac\ub97c \uacc4\uc0b0\ud55c\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#----------------\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "def dist_raw(v1, v2):\n",
      "    delta = v1 - v2\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "#-----------------\n",
      "\n",
      "import sys\n",
      "\n",
      "dist = dist_raw\n",
      "\n",
      "best_dist = sys.maxint\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== Post 0 with dist=1.41: Imaging databases store data.\n",
        "=== Post 1 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
        "\n",
        "=== Post 3 with dist=1.73: Imaging databases provide storage capabilities.\n",
        "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
        "Best post is 0 with dist=1.41\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uac8c\uc2dc\ubb3c 0\uc740 \uac19\uc740 \ub2e8\uc5b4\uac00 \uc5c6\uc5b4 \uac00\uc7a5 \ub2e4\ub974\ub2e4. \n",
      "\n",
      "\ub2e4\ub9cc, \uac8c\uc2dc\ubb3c 3, 4\uac00 \ub2e4\ub978 \uac83\uc774 \uba85\ucf8c\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\ub2e4. \ud574\ub2f9 \uc18d\uc131 \ubca1\ud130\ub97c \ubcf4\uba74 \ube48\ub3c4\uc5d0 \ub530\ub77c \ubca1\ud130 \ud06c\uae30\uc758 \ucc28\uc774\uac00 \ud06c\uae30 \ub54c\ubb38\uc5d0 \uc77c\uc5b4\ub098\ub294 \ud604\uc0c1\uc784\uc744 \uc54c \uc218 \uc788\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X_train.getrow(3).toarray())\n",
      "print(X_train.getrow(4).toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]]\n",
        "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc989, \ubca1\ud130\ub97c \uae38\uc774 \ub2e8\uc704\ub85c \uc815\uaddc\ud654\ud560 \ud544\uc694\uac00 \uc788\ub2e4. (\ubca1\ud130\uac00 \uc11c\ub85c \uc77c\uce58\ud558\ub294 \ubc29\ud5a5\uc774\ub77c\ub3c4 \ud06c\uae30 \ub54c\ubb38\uc5d0 \ub2e4\ub978 \ubca1\ud130\ub85c \uc778\uc2dd\ub41c\ub2e4.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\ub2e8\uc5b4 \uce74\uc6b4\ud2b8 \ubca1\ud130 \uc815\uaddc\ud654"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "dist_raw\ub97c \uc815\uaddc\ud654\ub41c \ubca1\ud130\ub85c \uacc4\uc0b0\ud558\ub3c4\ub85d \ud655\uc7a5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#----------------\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "\n",
      "    delta = v1_normalized - v2_normalized\n",
      "\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "#-----------------\n",
      "\n",
      "import sys\n",
      "\n",
      "dist = dist_norm\n",
      "\n",
      "best_dist = sys.maxint\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== Post 0 with dist=0.77: Imaging databases store data.\n",
        "=== Post 1 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
        "\n",
        "=== Post 3 with dist=0.86: Imaging databases provide storage capabilities.\n",
        "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
        "Best post is 0 with dist=0.77\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uac8c\uc2dc\ubb3c 3, 4\ub294 \uac19\uc740 \uc720\uc0ac\ub3c4\ub85c \uacc4\uc0b0\ub41c\ub2e4."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\ub35c \uc911\uc694\ud55c \ub2e8\uc5b4\uc758 \uc0ad\uc81c"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ubd88\uc6a9\uc5b4(stop word) - \uad00\uc0ac, \uc804\uce58\uc0ac, \ub300\uba85\uc0ac \ub4f1 \uc790\uc8fc \uc0ac\uc6a9\ub418\uc9c0\ub9cc \uac1c\ubcc4 \ub2e8\uc5b4\ub85c \uac80\uc0c9\ub418\uc9c0 \uc54a\ub294 \uc77c\ubc18\uc801\uc778 \ub2e8\uc5b4\n",
      "    - \uc0ad\uc81c \uc758\ubbf8 : \ub2e4\ub978 \ubb38\uc11c\ub4e4\uacfc \uad6c\ubcc4\ud558\ub294\ub370 \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\ub294 \ube48\ub3c4\uc218 \ub192\uc740 \ub2e8\uc5b4 \uc0ad\uc81c > \ubca1\ud130 \ud06c\uae30 \uac10\uc18c > \uacc4\uc0b0 \ud6a8\uc728 \uc99d\ub300\n",
      "    - \uc0ad\uc81c \ubc29\ubc95 : Python\uc5d0\uc11c\ub294 CountVectorizer\uc5d0 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc9c0\uc815\ud558\ub294 \uac83\uc73c\ub85c \uac04\ub2e8\ud558\uac8c \ucc98\ub9ac \uac00\ub2a5!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
      "\n",
      "sorted(vectorizer.get_stop_words())[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "['a',\n",
        " 'about',\n",
        " 'above',\n",
        " 'across',\n",
        " 'after',\n",
        " 'afterwards',\n",
        " 'again',\n",
        " 'against',\n",
        " 'all',\n",
        " 'almost',\n",
        " 'alone',\n",
        " 'along',\n",
        " 'already',\n",
        " 'also',\n",
        " 'although',\n",
        " 'always',\n",
        " 'am',\n",
        " 'among',\n",
        " 'amongst',\n",
        " 'amoungst']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc601\uc5b4 \ubd88\uc6a9\uc5b4 \uc911\uc5d0\uc11c \ucd5c\ucd08 20\uac1c\uc758 \ubd88\uc6a9\uc5b4\ub294 \uc704\uc640 \uac19\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts = [open(os.path.join(\"data/toy\", f)).read() for f in os.listdir(\"data/toy\")]\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
      "\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "\n",
      "print(vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 18\n",
        "[u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'learning', u'machine', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'toy']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc601\uc5b4 \ubd88\uc6a9\uc5b4\ub97c \uc81c\uac70\ud558\ub2c8 \ucc98\uc74c\ubcf4\ub2e4 7\uac1c\uc758 \ub2e8\uc5b4\uac00 \uc904\uc5c8\ub2e4. - \ubca1\ud130 \ud06c\uae30 \uac10\uc18c > \uac70\ub9ac \uacc4\uc0b0 \uc18d\ub3c4 \ud5a5\uc0c1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = \"imaging databases\"\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "\n",
      "print(new_post_vec.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc0c8\ub85c\uc6b4 \uac8c\uc2dc\uae00\uc758 \ubca1\ud130 \ud06c\uae30\ub3c4 \ub2e4\uc2dc \uc870\uc815\ud574 \uc8fc\uae30 \uc704\ud574 \ud55c \ubc88 \ub354 \ud638\ucd9c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#----------------\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "\n",
      "    delta = v1_normalized - v2_normalized\n",
      "\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "#-----------------\n",
      "\n",
      "import sys\n",
      "\n",
      "dist = dist_norm\n",
      "\n",
      "best_dist = sys.maxint\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== Post 0 with dist=0.77: Imaging databases store data.\n",
        "=== Post 1 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
        "\n",
        "=== Post 3 with dist=0.86: Imaging databases provide storage capabilities.\n",
        "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
        "Best post is 0 with dist=0.77\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ubd88\uc6a9\uc5b4\ub97c \uc81c\uc678\ud558\uba74 \uac8c\uc2dc\uae00 1, 2\uc758 \uc720\uc0ac\ub3c4\uac00 \uac19\uc544\uc9c4\ub2e4. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\uc5b4\uadfc \ucd94\ucd9c"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ud6a8\uc728\uc774 \ud06c\uac8c \uc99d\uac00\ud558\uc9c0\ub294 \uc54a\ub354\ub77c\ub3c4 \uc5b4\uadfc\uc744 \uae30\uc900\uc73c\ub85c \ud558\uba74 \ud615\ud0dc\uac00 \ub2e4\ub978 \ub2e8\uc5b4\ub3c4 \ubb36\uc744 \uc218 \uc788\uc5b4, \ubca1\ud130 \ud06c\uae30\ub97c \uc904\uc77c \uc218 \uc788\ub2e4.\n",
      "\n",
      "(\uc5b4\uadfc \ucd94\ucd9c\uc740 \ubc18\ub4dc\uc2dc \ud574\uc57c \ud558\ub294 \uc791\uc5c5\uc740 \uc544\ub2c8\ub2e4.)\n",
      "\n",
      "SciKit \uc790\uccb4\uc5d0\ub294 \uc5b4\uadfc \ucd94\ucd9c \ud568\uc218\uac00 \uc5c6\uc9c0\ub9cc, NLTK(Natural Language Toolkit)\uc758 \uc5b4\uadfc \ucd94\ucd9c\uae30(stemmer)\ub97c CountVectorizer\uc5d0 \ub123\uc5b4 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NLTK \uc124\uce58\uc640 \uc0ac\uc6a9"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc124\uce58 \ubc29\ubc95\n",
      "    - \uc0ac\uc774\ud2b8 : http://nltk.org/install.html [sudo pip install -U nltk]\n",
      "    - \ucc45 : \ub9e8 \ubc11 \uc8fc\uc11d [pip install nltk]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc124\uce58 \ud655\uc778\n",
      "\n",
      "NLTK\uc758 \uc5b8\uc5b4 \ucd94\ucd9c\uae30\uc5d0\ub294 \uc5ec\ub7ec \uc885\ub958\uac00 \uc788\ub2e4. \uc5b8\uc5b4\ub9c8\ub2e4 \ucd94\ucd9c \uaddc\uce59\ub3c4 \ub2e4\ub974\ub2e4. \n",
      "\uc601\uc5b4\uc758 \uacbd\uc6b0 SnowballStemmer\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.stem\n",
      "\n",
      "s = nltk.stem.SnowballStemmer('english')\n",
      "s.stem(\"graphics\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "u'graphic'"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.stem(\"imaging\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "u'imag'"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.stem(\"image\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "u'imag'"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.stem(\"imagination\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "u'imagin'"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.stem(\"imagine\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "u'imagin'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ub3d9\uc0ac\uc5d0 \uc5b4\uadfc \ucd94\ucd9c\uae30\ub97c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.stem(\"buys\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "u'buy'"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.stem(\"buying\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "u'buy'"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.stem(\"bought\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "u'bought'"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NLTK\uc640 \ubca1\ud130\ub77c\uc774\uc800 \ud655\uc7a5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc5b4\uadfc \ucd94\ucd9c\uc740 CountVectorizer\uc5d0 \uc785\ub825\ub418\uae30 \uc804\uc5d0 \uac8c\uc2dc\ubb3c\uc5d0\uc11c \ucd94\ucd9c\ud560 \ud544\uc694\uac00 \uc788\ub2e4.\n",
      "\n",
      "CountVectorizer \ud074\ub798\uc2a4\ub294 \uc804\ucc98\ub9ac\ub098 \ud1a0\ud070\ud654 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\uc790\uac00 \ud544\uc694\ud55c \ubd80\ubd84\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \uc218\ub2e8\uc744 \uc81c\uacf5\ud55c\ub2e4. - \uc0c1\uc18d \ud65c\uc6a9\n",
      "\n",
      "CountVectorizer\ub97c \uc0c1\uc18d\ud558\ub294 StemmedCountVectorizer \ud074\ub798\uc2a4(class)\ub97c \ub9cc\ub4e4\uace0 build_analyzer \uba54\uc18c\ub4dc\ub97c \uc624\ubc84\ub77c\uc774\ub529 \ud55c\ub2e4.\n",
      "\n",
      "    \uac8c\uc2dc\ubb3c\uc758 \ucc98\ub9ac\uacfc\uc815\n",
      "        1. \uc804\ucc98\ub9ac \ub2e8\uacc4\uc5d0\uc11c \uc18c\ubb38\uc790\ub85c \ubcc0\ud658(\uc0c1\uc704 \ud074\ub798\uc2a4)\n",
      "        2. \ud1a0\ud070\ud654 \ub2e8\uacc4\uc5d0\uc11c \ub2e8\uc5b4 \ucd94\ucd9c(\uc0c1\uc704 \ud074\ub798\uc2a4)\n",
      "        3. \uac01 \ub2e8\uc5b4\ub97c \uc5b4\uadfc\uc73c\ub85c \ubcc0\ud658"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\")\n",
      "% load rel_post_01_norm.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "[Errno 2] No such file or directory: '/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-25-9d1d1f5d33fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'load rel_post_01_norm.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03'"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "from utils import DATA_DIR\n",
      "\n",
      "TOY_DIR = os.path.join(DATA_DIR, \"toy\")\n",
      "posts = [open(os.path.join(TOY_DIR, f)).read() for f in os.listdir(TOY_DIR)]\n",
      "\n",
      "new_post = \"imaging databases\"\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "\n",
      "class StemmedCountVectorizer(CountVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "# vectorizer = CountVectorizer(min_df=1, stop_words='english',\n",
      "# preprocessor=stemmer)\n",
      "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
      "\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "print(new_post_vec, type(new_post_vec))\n",
      "print(new_post_vec.toarray())\n",
      "print(vectorizer.get_feature_names())\n",
      "\n",
      "\n",
      "def dist_raw(v1, v2):\n",
      "    delta = v1 - v2\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "\n",
      "    delta = v1_normalized - v2_normalized\n",
      "\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "dist = dist_norm\n",
      "\n",
      "best_dist = sys.maxsize\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 17\n",
        "(<1x17 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 2 stored elements in Compressed Sparse Row format>, <class 'scipy.sparse.csr.csr_matrix'>)\n",
        "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
        "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']\n",
        "=== Post 0 with dist=0.77: Imaging databases store data.\n",
        "=== Post 1 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "=== Post 2 with dist=0.63: Most imaging databases save images permanently.\n",
        "\n",
        "=== Post 3 with dist=0.86: Imaging databases provide storage capabilities.\n",
        "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
        "Best post is 2 with dist=0.63\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "imaging\uacfc images\uac00 \ud569\uccd0\uc838 \ubca1\ud130 \ud06c\uae30\uac00 1 \uc904\uc5b4\ub4e6\n",
      "\n",
      "imag \uc5b4\uadfc\uc774 2\ubc88 \ud3ec\ud568\ub418\ub294 \uac8c\uc2dc\ubb3c 2\uac00 \uac00\uc7a5 \uc720\uc0ac\ud574 \uc9d0"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\uac15\ud654\ub41c \ubd88\uc6a9\uc5b4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ubd88\uc6a9\uc5b4 \uac1c\ub150\uc758 \ud655\uc7a5 - \ub2e8\uc5b4 \ube48\ub3c4\uc218\uac00 \ub192\uc73c\uba74 \uc911\uc694\ud55c \uc6a9\uc5b4\ub77c\uace0 \ud560 \uc218 \uc788\uc9c0\ub9cc, \ubaa8\ub4e0 \uac8c\uc2dc\ubb3c\uc5d0 \ud56d\uc0c1 \ub098\ud0c0\ub098\ub294 \ub2e8\uc5b4 \uc790\uccb4\uac00 \uc758\ubbf8\uc788\ub294 \uac83\uc740 \uc544\ub2c8\ub2e4. (\uc608) \uac8c\uc2dc\uae00\uc758 '\uc81c\ubaa9')\n",
      "\n",
      "\ud574\uacb0\ucc45 1 : CountVectorizer\uc758 max_df \ub9e4\uac1c\ubcc0\uc218 \ud65c\uc6a9 - \ucd5c\uc801\uc758 \uac12\uc744 \ucc3e\uae30\uac00 \uc560\ub9e4\ud574\uc9c4\ub2e4.\n",
      "\n",
      "\ud574\uacb0\ucc45 2 : \uc6a9\uc5b4 \ube48\ub3c4-\uc5ed \ubb38\uc11c \ube48\ub3c4(TF-IDF, Term Frequency-Inverse Document Frequency) - \uc804\uccb4\uc801\uc73c\ub85c \ub9e4\uc6b0 \ub4dc\ubb3c\uc9c0\ub9cc \ud2b9\uc815 \uac8c\uc2dc\ubb3c\uc5d0\uc11c \uc790\uc8fc \ub098\ud0c0\ub098\ub294 \uc6a9\uc5b4\uac00 \uc911\uc694\ud55c \uc6a9\uc5b4\n",
      "    - TF : \ub2e8\uc5b4 \ube48\ub3c4\uc640 \uad00\ub828\n",
      "    - IDF : \ubb34\uc2dc\ud558\ub294 \ubd80\ubd84 \uace0\ub824"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\")\n",
      "% load tfidf.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "[Errno 2] No such file or directory: '/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-27-0698d35c10bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'load tfidf.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03'"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "\n",
      "def tfidf(t, d, D):\n",
      "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\n",
      "    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\n",
      "    return tf * idf\n",
      "\n",
      "\n",
      "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
      "D = [a, abb, abc]\n",
      "\n",
      "print(tfidf(\"a\", a, D))\n",
      "print(tfidf(\"b\", abb, D))\n",
      "print(tfidf(\"a\", abc, D))\n",
      "print(tfidf(\"b\", abc, D))\n",
      "print(tfidf(\"c\", abc, D))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n",
        "0.270310072072\n",
        "0.0\n",
        "0.135155036036\n",
        "0.366204096223\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "a\ub294 \ubaa8\ub4e0 \ubb38\uc11c\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc788\uc5b4\uc11c \uc5b4\ub5a4 \ubb38\uc11c\uc5d0\uc11c\ub3c4 \uc911\uc694\ud558\uc9c0 \uc54a\ub2e4.\n",
      "\n",
      "b\ub294 abc \ubb38\uc11c(\uac8c\uc2dc\uae00?)\ubcf4\ub2e4 abb\uc5d0\uc11c \uc790\uc8fc \ub098\uc624\ubbc0\ub85c abb \ubb38\uc11c\uc5d0\uc11c \ub354 \uc911\uc694\ud558\ub2e4.\n",
      "\n",
      "\uc2e4\uc81c \uc0ac\uc6a9\uc740 \uc27d\uc9c0 \uc54a\uc9c0\ub9cc, SciKit \ud328\ud0a4\uae30\uc5d0\ub294 CountVectorizer\ub97c \uc0c1\uc18d\ubc1b\uc740 TfidfVectorizer\uac00 \uc788\uc73c\ubbc0\ub85c \uc218\uc6d4\ud558\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\")\n",
      "% load rel_post_01_tfidf.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "[Errno 2] No such file or directory: '/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-29-20641145c4b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'load rel_post_01_tfidf.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03'"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "from utils import DATA_DIR\n",
      "\n",
      "TOY_DIR = os.path.join(DATA_DIR, \"toy\")\n",
      "posts = [open(os.path.join(TOY_DIR, f)).read() for f in os.listdir(TOY_DIR)]\n",
      "\n",
      "new_post = \"imaging databases\"\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(\n",
      "    min_df=1, stop_words='english', decode_error='ignore')\n",
      "\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "print(new_post_vec, type(new_post_vec))\n",
      "print(new_post_vec.toarray())\n",
      "print(vectorizer.get_feature_names())\n",
      "\n",
      "\n",
      "def dist_raw(v1, v2):\n",
      "    delta = v1 - v2\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "\n",
      "    delta = v1_normalized - v2_normalized\n",
      "\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "dist = dist_norm\n",
      "\n",
      "best_dist = sys.maxsize\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 17\n",
        "(<1x17 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 2 stored elements in Compressed Sparse Row format>, <class 'scipy.sparse.csr.csr_matrix'>)\n",
        "[[ 0.          0.          0.          0.          0.70710678  0.70710678\n",
        "   0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.          0.        ]]\n",
        "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']\n",
        "=== Post 0 with dist=0.92: Imaging databases store data.\n",
        "=== Post 1 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
        "\n",
        "=== Post 3 with dist=1.08: Imaging databases provide storage capabilities.\n",
        "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
        "Best post is 2 with dist=0.86\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ub2e8\uc5b4 \ube48\ub3c4\uc218 \ub300\uc2e0\uc5d0 \ub2e8\uc5b4\uc5d0 \ub300\ud55c TF-IDF \uac12\uc744 \ubca1\ud130\ud654\ud574\uc11c \ube44\uad50\ud55c \uacb0\uacfc\uc774\ub2e4."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\uc6b0\ub9ac\uc758 \uc131\ucde8\uc640 \ubaa9\ud45c"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ud14d\uc2a4\ud2b8 \uc804\ucc98\ub9ac \uacfc\uc815 : \ubca1\ud130\ud654 = \ub178\uc774\uc988\ud55c \ud14d\uc2a4\ud2b8 > \uc815\ud655\ud55c \uc18d\uc131 \uac12 \ud45c\ud604\n",
      "    1. \ud14d\uc2a4\ud2b8\uc758 \ud1a0\ud070\ud654 (\uc5b4\uadfc \ucd94\ucd9c \ub4f1 \ud3ec\ud568?)\n",
      "    2. \ub108\ubb34 \uc790\uc8fc \ub098\ud0c0\ub098\ub294 \ub2e8\uc5b4 \ubc30\uc81c\n",
      "    3. \ubbf8\ub798 \uac8c\uc2dc\ubb3c\uc5d0\uc11c \ub098\ud0c0\ub0a0 \ube48\ub3c4 \ub0ae\uc740 \ub2e8\uc5b4 \ubc30\uc81c\n",
      "    4. \ub0a8\uc740 \ub2e8\uc5b4 \uc138\uae30\n",
      "    5. \uc804\uccb4 \ud14d\uc2a4\ud2b8 \ub9d0\ubb49\uce58\ub97c \uace0\ub824\ud55c TF-IDF \uac12 \uacc4\uc0b0\n",
      "    \n",
      "\ub2e8\uc5b4 \uc8fc\uba38\ub2c8 \ubc29\ubc95\uc758 \ub2e8\uc810\n",
      "    1. \ub2e8\uc5b4 \uc0ac\uc774\uc758 \uad00\ub828\uc131 \ucde8\uc57d - \uc758\ubbf8\uac00 \ub2e4\ub978 \ubb38\uc7a5\uc774\ub77c\ub3c4 \ub2e8\uc5b4 \ube48\ub3c4\uac00 \uac19\uc73c\uba74 \uac19\uc740 \uc18d\uc131 \ubca1\ud130\ub97c \uac00\uc9d0\n",
      "    2. \ubd80\uc815\uc801\uc778 \uc758\ubbf8\ub97c \uc778\uc9c0\ud560 \uc218 \uc5c6\uc74c - \ubd80\uc815 \ub2e8\uc5b4\uc5d0 \uc758\ud574 \uc758\ubbf8\ub294 \ud06c\uac8c \ubcc0\ud558\uc9c0\ub9cc \uc18d\uc131 \ubca1\ud130\ub294 \ube44\uc2b7\ud568 (\ubc14\uc774\uadf8\ub7a8(bigrams), \ud2b8\ub77c\uc774\uadf8\ub7a8(trigrams)\uc73c\ub85c \uadf9\ubcf5 \uac00\ub2a5)\n",
      "    3. \uc624\uae30(\uc624\ud0c0)\uc5d0 \ucde8\uc57d - \uc544\ubb34\ub9ac \uc720\uc0ac\ud55c \ub2e8\uc5b4\ub77c\ub3c4 \uc11c\ub85c \ub2e4\ub978 \ub2e8\uc5b4\ub85c \ucde8\uae09"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\uad70\uc9d1\ud654"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uad70\uc9d1\ud654 \uc54c\uace0\ub9ac\uc998\n",
      "    1. \uc218\ud3c9 \uad70\uc9d1\ud654(flat clustering) : \uad70\uc9d1 \uac04\uc758 \uad00\uacc4\ub294 \uc5c6\uc774 \ub3d9\ub4f1\ud558\uac8c \ub098\ub214, \ud2b9\uc815 \uad70\uc9d1 \uac1c\uc218\ub97c \uc54c\ub824\uc918\uc57c \ud55c\ub2e4.\n",
      "    2. \uacc4\uce35 \uad70\uc9d1\ud654(hierarchical clustering) : \uad70\uc9d1 \uac04\uc5d0 \uacc4\uce35 \uad00\uacc4\uac00 \uc0dd\uae40, \uad70\uc9d1 \uac1c\uc218\ub97c \uc815\ud560 \uc218 \uc788\uc73c\ub098 \ud6a8\uc728 \ub5a8\uc5b4\uc9d0\n",
      "        \n",
      "SciKi\uc758 sklearn.cluster\uc5d0\ub294 \ub2e4\uc591\ud55c \uad70\uc9d1\ud654 \uc54c\uace0\ub9ac\uc998 \uc81c\uacf5\n",
      "\n",
      "\uad70\uc9d1\ud654 \uc54c\uace0\ub9ac\uc998\ub4e4\uc758 \uc7a5\ub2e8\uc810 : http://scikit-learn.org/dev/modules/clustering.html"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "K\ud3c9\uade0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K \ud3c9\uade0 \uc54c\uace0\ub9ac\uc998 - \uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130\ub97c k\uac1c\uc758 \uad70\uc9d1\uc73c\ub85c \ubb36\ub294 \uc54c\uace0\ub9ac\uc998, \uac01 \uad70\uc9d1\ub4e4\uacfc \uac70\ub9ac \ucc28\uc774\uc758 \ubd84\uc0b0\uc744 \ucd5c\uc18c\ud654\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ub3d9\uc791 - \uc218\ud3c9 \uad70\uc9d1\ud654 \uc54c\uace0\ub9ac\uc998\uc758 \ub300\ud45c\uc801\uc778 \ubc29\ubc95\n",
      "\n",
      "\uad70\uc9d1 \uac1c\uc218 num_clusters\ub85c \ucd08\uae30\ud654 > \uad70\uc9d1 \uc911\uc559\uc810(centroids : \ub3c4\uc2ec) \uad00\ub9ac\n",
      "\n",
      "\uc54c\uace0\ub9ac\uc998 \uc791\ub3d9 \ubc29\uc2dd\n",
      "    1. num_clusters\uc758 \uc18d\uc131 \ubca1\ud130\ub97c \uc911\uc559\uc810\uc73c\ub85c \uc124\uc815\n",
      "    2. \ud2b9\uc815 \ubc94\uc8fc\uc758 \ubaa8\ub4e0 \uac8c\uc2dc\ubb3c\uc758 \uc911\uc559\uc73c\ub85c \uc911\uc559\uc810 \uc774\ub3d9\n",
      "    3. \ubc94\uc8fc\uac00 \ubcc0\ud654\ub41c \uac8c\uc2dc\ubb3c\uc5d0 \ub300\ud55c \uad70\uc9d1 \uc9c0\uc815 \uac31\uc2e0\n",
      "    4. \uc911\uc559\uc810 \uc774\ub3d9\uc774 \uc77c\uc815 \uacbd\uacc4(SciKit \uae30\ubcf8 \uac12 = 0.0001) \uc544\ub798\ub85c \ub0b4\ub824\uac08 \ub54c\uae4c\uc9c0 \ubc18\ubcf5\n",
      "    \n",
      "\ub450 \ub2e8\uc5b4\ub85c \ub41c \uac8c\uc2dc\ubb3c\uc5d0 \ub300\ud55c \uc608    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\")\n",
      "% load plot_kmeans_example.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "# inspired by http://scikit-\n",
      "# learn.org/dev/auto_examples/cluster/plot_kmeans_digits.html#example-\n",
      "# cluster-plot-kmeans-digits-py\n",
      "\n",
      "import os\n",
      "import scipy as sp\n",
      "from scipy.stats import norm\n",
      "from matplotlib import pylab\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "from utils import CHART_DIR\n",
      "\n",
      "seed = 2\n",
      "sp.random.seed(seed)  # to reproduce the data later on\n",
      "\n",
      "num_clusters = 3\n",
      "\n",
      "\n",
      "def plot_clustering(x, y, title, mx=None, ymax=None, xmin=None, km=None):\n",
      "    pylab.figure(num=None, figsize=(8, 6))\n",
      "    if km:\n",
      "        pylab.scatter(x, y, s=50, c=km.predict(list(zip(x, y))))\n",
      "    else:\n",
      "        pylab.scatter(x, y, s=50)\n",
      "\n",
      "    pylab.title(title)\n",
      "    pylab.xlabel(\"Occurrence word 1\")\n",
      "    pylab.ylabel(\"Occurrence word 2\")\n",
      "\n",
      "    pylab.autoscale(tight=True)\n",
      "    pylab.ylim(ymin=0, ymax=1)\n",
      "    pylab.xlim(xmin=0, xmax=1)\n",
      "    pylab.grid(True, linestyle='-', color='0.75')\n",
      "\n",
      "    return pylab\n",
      "\n",
      "\n",
      "xw1 = norm(loc=0.3, scale=.15).rvs(20)\n",
      "yw1 = norm(loc=0.3, scale=.15).rvs(20)\n",
      "\n",
      "xw2 = norm(loc=0.7, scale=.15).rvs(20)\n",
      "yw2 = norm(loc=0.7, scale=.15).rvs(20)\n",
      "\n",
      "xw3 = norm(loc=0.2, scale=.15).rvs(20)\n",
      "yw3 = norm(loc=0.8, scale=.15).rvs(20)\n",
      "\n",
      "x = sp.append(sp.append(xw1, xw2), xw3)\n",
      "y = sp.append(sp.append(yw1, yw2), yw3)\n",
      "\n",
      "i = 1\n",
      "plot_clustering(x, y, \"Vectors\")\n",
      "pylab.savefig(os.path.join(CHART_DIR, \"1400_03_0%i.png\" % i))\n",
      "pylab.clf()\n",
      "\n",
      "i += 1\n",
      "\n",
      "# 1 iteration ####################\n",
      "\n",
      "mx, my = sp.meshgrid(sp.arange(0, 1, 0.001), sp.arange(0, 1, 0.001))\n",
      "\n",
      "km = KMeans(init='random', n_clusters=num_clusters, verbose=1,\n",
      "            n_init=1, max_iter=1,\n",
      "            random_state=seed)\n",
      "km.fit(sp.array(list(zip(x, y))))\n",
      "\n",
      "Z = km.predict(sp.c_[mx.ravel(), my.ravel()]).reshape(mx.shape)\n",
      "\n",
      "plot_clustering(x, y, \"Clustering iteration 1\", km=km)\n",
      "pylab.imshow(Z, interpolation='nearest',\n",
      "             extent=(mx.min(), mx.max(), my.min(), my.max()),\n",
      "             cmap=pylab.cm.Blues,\n",
      "             aspect='auto', origin='lower')\n",
      "\n",
      "c1a, c1b, c1c = km.cluster_centers_\n",
      "pylab.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
      "              marker='x', linewidth=2, s=100, color='black')\n",
      "pylab.savefig(os.path.join(CHART_DIR, \"1400_03_0%i.png\" % i))\n",
      "pylab.clf()\n",
      "\n",
      "i += 1\n",
      "\n",
      "# 2 iterations ####################\n",
      "km = KMeans(init='random', n_clusters=num_clusters, verbose=1,\n",
      "            n_init=1, max_iter=2,\n",
      "            random_state=seed)\n",
      "km.fit(sp.array(list(zip(x, y))))\n",
      "\n",
      "Z = km.predict(sp.c_[mx.ravel(), my.ravel()]).reshape(mx.shape)\n",
      "\n",
      "plot_clustering(x, y, \"Clustering iteration 2\", km=km)\n",
      "pylab.imshow(Z, interpolation='nearest',\n",
      "             extent=(mx.min(), mx.max(), my.min(), my.max()),\n",
      "             cmap=pylab.cm.Blues,\n",
      "             aspect='auto', origin='lower')\n",
      "\n",
      "c2a, c2b, c2c = km.cluster_centers_\n",
      "pylab.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
      "              marker='x', linewidth=2, s=100, color='black')\n",
      "\n",
      "pylab.gca().add_patch(\n",
      "    pylab.Arrow(c1a[0], c1a[1], c2a[0] - c1a[0], c2a[1] - c1a[1], width=0.1))\n",
      "pylab.gca().add_patch(\n",
      "    pylab.Arrow(c1b[0], c1b[1], c2b[0] - c1b[0], c2b[1] - c1b[1], width=0.1))\n",
      "pylab.gca().add_patch(\n",
      "    pylab.Arrow(c1c[0], c1c[1], c2c[0] - c1c[0], c2c[1] - c1c[1], width=0.1))\n",
      "\n",
      "pylab.savefig(os.path.join(CHART_DIR, \"1400_03_0%i.png\" % i))\n",
      "pylab.clf()\n",
      "\n",
      "i += 1\n",
      "\n",
      "# 3 iterations ####################\n",
      "km = KMeans(init='random', n_clusters=num_clusters, verbose=1,\n",
      "            n_init=1, max_iter=10,\n",
      "            random_state=seed)\n",
      "km.fit(sp.array(list(zip(x, y))))\n",
      "\n",
      "Z = km.predict(sp.c_[mx.ravel(), my.ravel()]).reshape(mx.shape)\n",
      "\n",
      "plot_clustering(x, y, \"Clustering iteration 10\", km=km)\n",
      "pylab.imshow(Z, interpolation='nearest',\n",
      "             extent=(mx.min(), mx.max(), my.min(), my.max()),\n",
      "             cmap=pylab.cm.Blues,\n",
      "             aspect='auto', origin='lower')\n",
      "\n",
      "pylab.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
      "              marker='x', linewidth=2, s=100, color='black')\n",
      "pylab.savefig(os.path.join(CHART_DIR, \"1400_03_0%i.png\" % i))\n",
      "pylab.clf()\n",
      "\n",
      "i += 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialization complete\n",
        "Iteration  0, inertia 4.749\n",
        "Initialization complete"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  0, inertia 4.749\n",
        "Iteration  1, inertia 3.379\n",
        "Initialization complete"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  0, inertia 4.749\n",
        "Iteration  1, inertia 3.379\n",
        "Iteration  2, inertia 2.600\n",
        "Iteration  3, inertia 2.497\n",
        "Iteration  4, inertia 2.447\n",
        "Converged at iteration 4\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x4435d90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x48d9bd0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x4a67910>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x4a5a290>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc801\uc6a9 \ubc29\ubc95\n",
      "    1. \uad70\uc9d1\ud654 \ud6c4, \uad70\uc9d1 \uc911\uc559\uc810\ub4e4\uacfc \uadf8 \ubca1\ud130\ub97c \uae30\ub85d\n",
      "    2. \uc0c8 \uac8c\uc2dc\ubb3c\uc774 \uc785\ub825\ub418\uba74 \ubca1\ud130\ud654\ud574\uc11c \ubaa8\ub4e0 \uad70\uc9d1 \uc911\uc559\uc810 \ubca1\ud130\uc640 \ube44\uad50\n",
      "    3. \uac70\ub9ac\uac00 \uac00\uc7a5 \uc791\uc740 \uad70\uc9d1 \uc911\uc559\uc810\uc744 \ud3ec\ud568\ud558\ub294 \uad70\uc9d1\uc73c\ub85c \uc9c0\uc815"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\uc6b0\ub9ac\uc758 \ubc1c\uc0c1\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc5bb\uae30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "20newsgroup datasets\n",
      "    - \uae30\uacc4 \ud559\uc2b5\uc744 \uc704\ud55c \ud45c\uc900\uc801\uc778 \ub370\uc774\ud130 [20newsgroup dataset] : 20\uac1c\uc758 \ub2e4\ub978 \ub274\uc2a4 \uadf8\ub8f9\uc5d0 18,826\uac1c\uc758 \uac8c\uc2dc\ubb3c \ud3ec\ud568\n",
      "    - \uad70\uc9d1\ud654\ub97c \ud14c\uc2a4\ud2b8\uae30 \uc88b\uc740 \ub370\uc774\ud130 \uc9d1\ud569 : \uac01 \ub274\uc2a4 \uadf8\ub8f9\uc744 \uad70\uc9d1\uc73c\ub85c \uac00\uc815\ud558\uc5ec \uad00\ub828 \uac8c\uc2dc\ubb3c \ucc3e\ub294 \uc54c\uace0\ub9ac\uc998\uc758 \uc791\ub3d9\uc744 \ud14c\uc2a4\ud2b8\n",
      "    - \ubc1b\uae30 : [http://people.csail.mit.edu/jrennie/20Newsgroups], [http://mlcomp.org/datasets/379] - \ubb34\ub8cc \ub4f1\ub85d \ud544\uc694\n",
      "    - \uad6c\uc131 : zip file - \uc555\ucd95 \ud6c4 '379' \ud3f4\ub354\uc5d0 \uba54\ud0c0\ub370\uc774\ud130 \ud30c\uc77c \ubc0f test, train, raw\uc758 3\uac1c\uc758 \ud558\uc704 \ud3f4\ub354\ub85c \uad6c\uc131\n",
      "    - \uc5f4\uae30 : Scikit\uc740 \uc774 \ub370\uc774\ud130\ub97c \uc704\ud55c \uc0ac\uc6a9\uc790 \ub85c\ub354(custom loader)\ub97c \uac16\uace0 \uc788\uc5b4 \ub370\uc774\ud130 \ub85c\ub529\uc774 \uc27d\ub2e4.\n",
      "    - \uc0ac\uc6a9 : data\uac00 \uc788\ub294 \uc704\uce58\ub97c Scikit\uc5d0 \uc54c\ub824\uc918\uc57c \ud568 - MLCOMP_DATASETS_HOME \ud658\uacbd \ubcc0\uc218 or mlcomp_root\uc5d0 \uc704\uce58\ub97c \uba85\uc2dc"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.datasets\n",
      "\n",
      "from utils import DATA_DIR\n",
      "\n",
      "data = sklearn.datasets.load_mlcomp('20news-18828', mlcomp_root=DATA_DIR)\n",
      "\n",
      "print(data.filenames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Could not find dataset with metadata line: name: 20news-18828",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-32-a19cdaa47859>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDATA_DIR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_mlcomp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'20news-18828'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlcomp_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/site-packages/sklearn/datasets/mlcomp.pyc\u001b[0m in \u001b[0;36mload_mlcomp\u001b[1;34m(name_or_id, set_, mlcomp_root, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             raise ValueError(\"Could not find dataset with metadata line: \" +\n\u001b[1;32m---> 88\u001b[1;33m                              expected_name_line)\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;31m# loading the dataset metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Could not find dataset with metadata line: name: 20news-18828"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(len(data.filenames))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18828\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ud6c8\ub828 \ub370\uc774\ud130\uc14b\uacfc \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b \uc9c0\uc815"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data = sklearn.datasets.load_mlcomp('20news-18828', 'train', mlcomp_root=DATA_DIR)\n",
      "print(len(train_data.filenames))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13180\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data = sklearn.datasets.load_mlcomp('20news-18828', 'test', mlcomp_root=DATA_DIR)\n",
      "print(len(test_data.filenames))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5648\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc77c\ubd80 \ub274\uc2a4 \uadf8\ub8f9\uc73c\ub85c \uadf8\ub8f9 \uc81c\ud55c : \uc804\uccb4 \uc2e4\ud5d8 \uc8fc\uae30 \ub2e8\ucd95 - categories \ub9e4\uac1c\ubcc0\uc218 \uc124\uc815"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
      "train_data = sklearn.datasets.load_mlcomp('20news-18828', 'train', mlcomp_root=DATA_DIR, categories=groups)\n",
      "print(len(train_data.filenames))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4119\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\uac8c\uc2dc\ubb3c \uad70\uc9d1\ud654"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc2e4\uc81c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 \ub178\uc774\uc988\ub97c \uc81c\uac70\ud574\uc57c \ud55c\ub2e4. \n",
      "    - UnicodeDecodeError \ub4f1, \uc720\ud6a8\ud558\uc9c0 \uc54a\ub294 \ubb38\uc790\ub4e4\uc5d0 \uc758\ud55c \uc624\ub958\ub97c \ubb34\uc2dc\ud560 \ud544\uc694\uc788\ub2e4.\n",
      "    - Vectorizer\uc5d0 error\ub97c \ubb34\uc2dc\ud560\uc9c0 \uc9c0\uc815 : \ucc45\uc5d0 \uc788\ub294 charset_error\ub294 decode_error\ub85c \ubcc0\uacbd\ud574\uc57c \ud568 - \ucd5c\uc2e0 \ubc84\uc804"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "    \n",
      "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5, stop_words='english', decode_error='ignore')\n",
      "vectorized = vectorizer.fit_transform(train_data.data)\n",
      "num_samples, num_features = vectorized.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 4119, #features: 4751\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uacb0\uacfc\ub85c 4,119\uac1c\uc758 \uac8c\uc2dc\ubb3c\uc5d0\uc11c 4,751\uac1c\uc758 \uc18d\uc131 \ucc28\uc6d0(\ub2e8\uc5b4 \uc218?)\uc744 \ucd94\ucd9c - K\ud3c9\uade0\uc5d0 \uc0ac\uc6a9\ub420 \ub370\uc774\ud130\uc14b\n",
      "\n",
      "\uad70\uc9d1 \uac1c\uc218\ub97c 50\uc73c\ub85c \uc9c0\uc815\ud558\uc5ec \uc2e4\ud589\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_clusters = 50\n",
      "from sklearn.cluster import KMeans\n",
      "km = KMeans(n_clusters=num_clusters, init='random', n_init=1, verbose=1)\n",
      "km.fit(vectorized)\n",
      "\n",
      "print(km.labels_)\n",
      "km.labels_.shape\n",
      "\n",
      "print(km.cluster_centers_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialization complete\n",
        "Iteration  0, inertia 6993.898"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  1, inertia 3792.147"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  2, inertia 3744.388"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  3, inertia 3714.907\n",
        "Iteration  4, inertia 3698.410"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  5, inertia 3689.491\n",
        "Iteration  6, inertia 3684.800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  7, inertia 3682.478\n",
        "Iteration  8, inertia 3680.729"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  9, inertia 3679.239\n",
        "Iteration 10, inertia 3677.585"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 11, inertia 3676.519\n",
        "Iteration 12, inertia 3676.102"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 13, inertia 3675.304\n",
        "Iteration 14, inertia 3674.764"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 15, inertia 3674.500\n",
        "Iteration 16, inertia 3674.157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 17, inertia 3674.002\n",
        "Iteration 18, inertia 3673.943"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 19, inertia 3673.872\n",
        "Iteration 20, inertia 3673.807"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 21, inertia 3673.729\n",
        "Iteration 22, inertia 3673.705"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 23, inertia 3673.699\n",
        "Converged at iteration 23\n",
        "[11 16 49 ..., 44 44  3]\n",
        "[[  1.10848204e-02   2.11224770e-04   3.33575745e-03 ...,   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00]\n",
        " [  6.23263965e-03   9.62748274e-05   3.64240570e-03 ...,   0.00000000e+00\n",
        "    6.36237849e-04   4.55984129e-04]\n",
        " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00]\n",
        " ..., \n",
        " [  0.00000000e+00   2.89937292e-03   0.00000000e+00 ...,   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00]\n",
        " [  2.56256138e-03   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00]\n",
        " [  0.00000000e+00   0.00000000e+00   1.26103707e-04 ...,   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00]]\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "km.labels_ - \ubca1\ud130\ud654 \uac8c\uc2dc\ubb3c\uc758 \uad70\uc9d1 \ub77c\ubca8 (50\uac1c\uc758 \uadf8\ub8f9 \uc911 \ud558\ub098\uc758 \uac12?)\n",
      "\n",
      "km.labels_.shape - 4,119\uac1c\uc758 \uac8c\uc2dc\ubb3c\uc740 \uac01\uac01 1~50\uac1c\uc758 \uadf8\ub8f9 \uc911 \ud558\ub098\uc5d0 \uc18c\uc18d\n",
      "\n",
      "km.cluster_centers_ - \uad70\uc9d1\uc758 \uc911\uc559\uc810\n",
      "\n",
      "km.predict - \uc0c8\ub85c\uc6b4 \uac8c\uc2dc\ubb3c\uc5d0 \ub300\ud574 \uad70\uc9d1 \uc9c0\uc815"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\ucd08\uae30 \ub3c4\uc804\uacfc\uc81c \ud574\uacb0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc0c8 \uac8c\uc2dc\ubb3c\uc744 \ubca1\ud130\ud654\ud55c \ud6c4 km.predic\ub97c \uc774\uc6a9\ud558\uc5ec \uc5b4\ub290 \uad70\uc9d1\uc5d0 \uc18d\ud558\ub294\uc9c0 \uacb0\uc815"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = \\\n",
      "    \"\"\"Disk drive problems. Hi, I have a problem with my hard disk.\n",
      "After 1 year it is working only sporadically now.\n",
      "I tried to format it, but now it doesn't boot any more.\n",
      "Any ideas? Thanks.\n",
      "\"\"\"\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "new_post_label = km.predict(new_post_vec)[0]\n",
      "\n",
      "print(new_post_label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uad70\uc9d1\ud654 \uc774\ud6c4\ubbc0\ub85c \ubaa8\ub4e0 \uac8c\uc2dc\ubb3c\uc774 \uc544\ub2cc \uac19\uc740 \uad70\uc9d1 \ub0b4\uc5d0\uc11c \uac8c\uc2dc\ubb3c\uc758 \uc720\uc0ac\ub3c4 \uce21\uc815"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "\n",
      "similar_indices = (km.labels_ == new_post_label).nonzero()[0]\n",
      "\n",
      "similar = []\n",
      "for i in similar_indices:\n",
      "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
      "    similar.append((dist, train_data.data[i]))\n",
      "\n",
      "similar = sorted(similar)\n",
      "print(\"Count similar: %i\" % len(similar))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Count similar: 143\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "similar_indices - boolean \ubc30\uc5f4 \uc911 True\uac00 \ud3ec\ud568\ub41c \ubc30\uc5f4\uc744 \uac00\uc9c4\ub2e4. - \uc0c8 \uac8c\uc2dc\ubb3c\uacfc \uac19\uc740 \uadf8\ub8f9\ub9cc \ub0a8\uac8c\ub428\n",
      "\n",
      "\uc0c8\ub85c\uc6b4 \uac8c\uc2dc\ubb3c\uc744 \ud3ec\ud568\ud558\ub294 \uad70\uc9d1\uc5d0 \uc788\ub294 \uac8c\uc2dc\ubb3c \uc218 : 149 / 143 - \ub9e4\ubc88 \ub2ec\ub77c\uc9d0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_at_1 = similar[0]\n",
      "show_at_2 = similar[int(len(similar) / 10)]\n",
      "show_at_3 = similar[int(len(similar) / 2)]\n",
      "\n",
      "print(\"=== #1 ===\")\n",
      "print(show_at_1)\n",
      "print()\n",
      "\n",
      "print(\"=== #2 ===\")\n",
      "print(show_at_2)\n",
      "print()\n",
      "\n",
      "print(\"=== #3 ===\")\n",
      "print(show_at_3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== #1 ===\n",
        "(1.0276440026439035, \"From: rogntorb@idt.unit.no (Torbj|rn Rognes)\\nSubject: Adding int. hard disk drive to IIcx\\n\\nI haven't seen much info about how to add an extra internal disk to a\\nmac. We would like to try it, and I wonder if someone had some good\\nadvice.\\n\\nWe have a Mac IIcx with the original internal Quantum 40MB hard disk,\\nand an unusable floppy drive. We also have a new spare Connor 40MB\\ndisk which we would like to use. The idea is to replace the broken\\nfloppy drive with the new hard disk, but there seems to be some\\nproblems:\\n\\nThe internal SCSI cable and power cable inside the cx has only\\nconnectors for one single hard disk drive.\\n\\nIf I made a ribbon cable and a power cable with three connectors each\\n(1 for motherboard, 1 for each of the 2 disks), would it work?\\n\\nIs the IIcx able to supply the extra power to the extra disk?\\n\\nWhat about terminators? I suppose that i should remove the resistor\\npacks from the disk that is closest to the motherboard, but leave them\\ninstalled in the other disk.\\n\\nThe SCSI ID jumpers should also be changed so that the new disk gets\\nID #1. The old one should have ID #0.\\n\\nIt is no problem for us to remove the floppy drive, as we have an\\nexternal floppy that we can use if it won't boot of the hard disk.\\n\\nThank you!\\n\\n----------------------------------------------------------------------\\nTorbj|rn Rognes                            Email: rogntorb@idt.unit.no\\n\")\n",
        "()\n",
        "=== #2 ===\n",
        "(1.1763968624008132, 'From: mfeldman@bu.edu (Michael Feldman)\\nSubject: Floptical Kills Superdrive\\n\\nI recently bought a PLI 21mgbyte floptical drive, and I was very happy \\nwith it until I tried to use it to format a 1.4 HD diskette. I put the\\nHD floppy in my Superdrive to check that the floptical had formatted it\\ncorrectly, and now my Superdrive refuses to recognize ANY floppy (it says\\n\"this disk is unreadable\" and asks if I want to format it) even original\\nsystems floppies from Apple. Nor will it format the disks if I try to\\n(\"initialization failed!\")  Strangely enough the floptical still reads\\nboth the 21 MB and 1.4 HD disks, but I cant look at my 800k floppies, and\\nif I have a crash I\\'m screwed because the Floptical can\\'t be used as a\\nstart-up disk.  PLI has been unresponsive.  Any ideas? Has this happened\\nto anyone before? I was looking for an inexpensive storage solution, and\\nnow I am looking at an expensive repair.  Help! respond to this thread, or\\nemail mfeldman@acs.bu.edu\\n')\n",
        "()\n",
        "=== #3 ===\n",
        "(1.291963994784497, \"From: blaine@catt.ncsu.edu (Grey Mull)\\nSubject: HELP formatting NT disk array\\n\\n\\t\\n\\tI have a Northern Telecom disk array dated 1987 that has two 253MB drives\\nunits in it and I cannot get it formatted.  I set the SCSI ID on 1 and my\\nsoftware recognizes the unit, but I cannot mount it or anything - do I have\\nto use BOTH drives in the array? \\n\\n\\tAny help with these drives or possibly newer software than what I'm using\\n(FWB HDT1.0 and 1.1) will be greatly appreciated!\\n\\n-grey\\n\\n\\n Grey Mull                   ******************************************\\nblaine@catt.ncsu.edu         * Smoke pot, dodge the draft, cheat on   *\\ngbmull@eos.ncsu.edu          * your wife, become President ...        *\\nNCSU CATT Program            ************THE AMERICAN DREAM************\\n\")\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ucc98\uc74c\ubd80\ud130 \uac00\uc7a5 \uc720\uc0ac\ud55c \uac8c\uc2dc\ubb3c \uac00\uc7a5 \ub35c \uc720\uc0ac\ud55c \uac8c\uc2dc\ubb3c, \uc911\uac04 \uc815\ub3c4 \uc720\uc0ac\ud55c \uac8c\uc2dc\ubb3c - \uacb0\uacfc\ub294 \ud56d\uc0c1 \uac19\uc9c0\ub294 \uc54a\ub2e4? (\ucc45\uacfc\ub294 \ub2e4\ub974\ub2e4? - \ubc84\uc804 \ubb38\uc81c?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\")\n",
      "% load rel_post_20news_test.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import sklearn.datasets\n",
      "import scipy as sp\n",
      "\n",
      "new_post = \\\n",
      "    \"\"\"Disk drive problems. Hi, I have a problem with my hard disk.\n",
      "After 1 year it is working only sporadically now.\n",
      "I tried to format it, but now it doesn't boot any more.\n",
      "Any ideas? Thanks.\n",
      "\"\"\"\n",
      "\n",
      "print(\"\"\"\\\n",
      "Dear reader of the 1st edition of 'Building Machine Learning Systems with Python'!\n",
      "For the 2nd edition we introduced a couple of changes that will result into\n",
      "results that differ from the results in the 1st edition.\n",
      "E.g. we now fully rely on scikit's fetch_20newsgroups() instead of requiring\n",
      "you to download the data manually from MLCOMP.\n",
      "If you have any questions, please ask at http://www.twotoreal.com\n",
      "\"\"\")\n",
      "\n",
      "# mine\n",
      "from utils import DATA_DIR\n",
      "all_data = sklearn.datasets.load_mlcomp('20news-18828', mlcomp_root=DATA_DIR)\n",
      "\n",
      "print(\"Number of total posts: %i\" % len(all_data.filenames))\n",
      "# Number of total posts: 18846\n",
      "\n",
      "groups = [\n",
      "    'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
      "    'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
      "\n",
      "# mine\n",
      "train_data = sklearn.datasets.load_mlcomp('20news-18828', 'train', mlcomp_root=DATA_DIR, categories=groups)\n",
      "\n",
      "print(\"Number of training posts in tech groups:\", len(train_data.filenames))\n",
      "# Number of training posts in tech groups: 3529\n",
      "\n",
      "labels = train_data.target\n",
      "num_clusters = 50  # sp.unique(labels).shape[0]\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
      "                                    stop_words='english', decode_error='ignore'\n",
      "                                    )\n",
      "\n",
      "vectorized = vectorizer.fit_transform(train_data.data)\n",
      "num_samples, num_features = vectorized.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "# samples: 3529, #features: 4712\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "km = KMeans(n_clusters=num_clusters, n_init=1, verbose=1, random_state=3)\n",
      "clustered = km.fit(vectorized)\n",
      "\n",
      "print(\"km.labels_=%s\" % km.labels_)\n",
      "# km.labels_=[ 6 34 22 ...,  2 21 26]\n",
      "\n",
      "print(\"km.labels_.shape=%s\" % km.labels_.shape)\n",
      "# km.labels_.shape=3529\n",
      "\n",
      "from sklearn import metrics\n",
      "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
      "# Homogeneity: 0.400\n",
      "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
      "# Completeness: 0.206\n",
      "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
      "# V-measure: 0.272\n",
      "print(\"Adjusted Rand Index: %0.3f\" %\n",
      "      metrics.adjusted_rand_score(labels, km.labels_))\n",
      "# Adjusted Rand Index: 0.064\n",
      "print(\"Adjusted Mutual Information: %0.3f\" %\n",
      "      metrics.adjusted_mutual_info_score(labels, km.labels_))\n",
      "# Adjusted Mutual Information: 0.197\n",
      "print((\"Silhouette Coefficient: %0.3f\" %\n",
      "       metrics.silhouette_score(vectorized, labels, sample_size=1000)))\n",
      "# Silhouette Coefficient: 0.006\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "new_post_label = km.predict(new_post_vec)[0]\n",
      "\n",
      "similar_indices = (km.labels_ == new_post_label).nonzero()[0]\n",
      "\n",
      "similar = []\n",
      "for i in similar_indices:\n",
      "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
      "    similar.append((dist, train_data.data[i]))\n",
      "\n",
      "similar = sorted(similar)\n",
      "print(\"Count similar: %i\" % len(similar))\n",
      "\n",
      "show_at_1 = similar[0]\n",
      "show_at_2 = similar[int(len(similar) / 10)]\n",
      "show_at_3 = similar[int(len(similar) / 2)]\n",
      "\n",
      "print(\"=== #1 ===\")\n",
      "print(show_at_1)\n",
      "print()\n",
      "\n",
      "print(\"=== #2 ===\")\n",
      "print(show_at_2)\n",
      "print()\n",
      "\n",
      "print(\"=== #3 ===\")\n",
      "print(show_at_3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dear reader of the 1st edition of 'Building Machine Learning Systems with Python'!\n",
        "For the 2nd edition we introduced a couple of changes that will result into\n",
        "results that differ from the results in the 1st edition.\n",
        "E.g. we now fully rely on scikit's fetch_20newsgroups() instead of requiring\n",
        "you to download the data manually from MLCOMP.\n",
        "If you have any questions, please ask at http://www.twotoreal.com\n",
        "\n",
        "Number of total posts: 18828"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Number of training posts in tech groups:', 4119)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "#samples: 4119, #features: 4751"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Initialization complete"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  0, inertia 6749.613\n",
        "Iteration  1, inertia 3749.246"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  2, inertia 3707.588"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  3, inertia 3689.871\n",
        "Iteration  4, inertia 3680.498"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  5, inertia 3675.710\n",
        "Iteration  6, inertia 3672.737"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  7, inertia 3671.088\n",
        "Iteration  8, inertia 3670.509"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  9, inertia 3670.285\n",
        "Iteration 10, inertia 3670.045"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 11, inertia 3669.833\n",
        "Iteration 12, inertia 3669.631"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 13, inertia 3669.203\n",
        "Iteration 14, inertia 3669.024"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 15, inertia 3669.007\n",
        "Iteration 16, inertia 3668.997"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 17, inertia 3668.992\n",
        "Converged at iteration 17\n",
        "km.labels_=[17 35 11 ..., 38 38 23]\n",
        "km.labels_.shape=4119\n",
        "Homogeneity: 0.423"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completeness: 0.220\n",
        "V-measure: 0.289\n",
        "Adjusted Rand Index: 0.093\n",
        "Adjusted Mutual Information: 0.213\n",
        "Silhouette Coefficient: 0.006"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Count similar: 59\n",
        "=== #1 ===\n",
        "(1.0276440026439035, \"From: rogntorb@idt.unit.no (Torbj|rn Rognes)\\nSubject: Adding int. hard disk drive to IIcx\\n\\nI haven't seen much info about how to add an extra internal disk to a\\nmac. We would like to try it, and I wonder if someone had some good\\nadvice.\\n\\nWe have a Mac IIcx with the original internal Quantum 40MB hard disk,\\nand an unusable floppy drive. We also have a new spare Connor 40MB\\ndisk which we would like to use. The idea is to replace the broken\\nfloppy drive with the new hard disk, but there seems to be some\\nproblems:\\n\\nThe internal SCSI cable and power cable inside the cx has only\\nconnectors for one single hard disk drive.\\n\\nIf I made a ribbon cable and a power cable with three connectors each\\n(1 for motherboard, 1 for each of the 2 disks), would it work?\\n\\nIs the IIcx able to supply the extra power to the extra disk?\\n\\nWhat about terminators? I suppose that i should remove the resistor\\npacks from the disk that is closest to the motherboard, but leave them\\ninstalled in the other disk.\\n\\nThe SCSI ID jumpers should also be changed so that the new disk gets\\nID #1. The old one should have ID #0.\\n\\nIt is no problem for us to remove the floppy drive, as we have an\\nexternal floppy that we can use if it won't boot of the hard disk.\\n\\nThank you!\\n\\n----------------------------------------------------------------------\\nTorbj|rn Rognes                            Email: rogntorb@idt.unit.no\\n\")\n",
        "()\n",
        "=== #2 ===\n",
        "(1.1416438956926289, \"From: chrisw@yang.earlham.edu\\nSubject: accelerated Mac Plus problems\\n\\nHello\\n\\tI recently accelerated my Mac Plus (MicroMac 25MHz accelerator) \\nand now I can't get my Mac to boot off of the hard disk.  It boots fine \\nfrom a floppy, and I can mount the hard disk using SCSI probe, but I \\ncannot get it to boot from the hard drive.  I installed a new driver, so I \\ndon't think that is the problem.  \\n\\tThis poses a rather large problem.  I only have 4 Meg of RAM, and \\nI need to run Mathematica, which requires 5 Meg.  I was hoping to use \\nsystem 7 virtual memory so that I could run Mathematica.  However, I can't \\nrun system 7 from a floppy, so I can't get enough RAM.  \\n\\tAny suggestions? Thanks for your time--\\n\\t\\t\\t\\t\\t\\tChrisw@yang.earlham.edu\\n\")\n",
        "()\n",
        "=== #3 ===\n",
        "(1.2601096750163272, \"From: dlc@umcc.umcc.umich.edu (David Claytor)\\nSubject: Re: When is Apple going to ship CD300i's?\\n\\nIn article <1r00fdINNddt@senator-bedfellow.MIT.EDU> thewho@athena.mit.edu (Derek A Fong) writes:\\n>\\n>Interestingly enough, the CDROM 300i that came with my Quadra 800 has \\n>only 8 disks:\\n>\\n>1. System Install\\n>2. Kodak Photo CD sampler\\n>3. Alice to Ocean\\n>4. CDROM Titles\\n>5. Application Demos\\n>6. Mozart: Dissonant Quartet\\n>7. Nautilus\\n>8. Apple Chronicles\\n>\\n>Has anyone else noticed that they got less than everyone seems to be\\n>getting with the external?  What I really feel I missed out on is what\\n>is supposed to a fantastic Games demo disk.\\n>\\n>I have heard that people have gotten up to 9-10 disks with their drive.\\n>I assume they get the 8 titles above plus Cinderella and the Games Demo CDROM.\\n>\\n>any comments and experiences?  Should I call Apple to complain? =)\\n>\\n>Derek\\n>\\n>\\n>thewho@plume.mit.edu\\n\\n\\nWhat I did NOT get with my drive (CD300i) is the System Install CD you\\nlisted as #1.  Any ideas about how I can get one?  I bought my IIvx 8/120\\nfrom Direct Express in Chicago (no complaints at all -- good price & good\\nservice).\\n\\nBTW, I've heard that the System Install CD can be used to boot the mac;\\nhowever, my drive will NOT accept a CD caddy is the machine is off.  How can\\nyou boot with it then?\\n\\n--Dave\\n\\n-- \\n                           dlc@umcc.ais.org  313.485.3394\\n\\n\")\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\ub178\uc774\uc988\uc758 \ub610 \ub2e4\ub978 \uc2dc\uac01"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uad70\uc9d1\ud654\uac00 \uc644\ubcbd\ud55c\uac00? - 'com.graphics' \ub274\uc2a4 \uadf8\ub8f9\uc758 \uac8c\uc2dc\ubb3c\ub3c4 \uad70\uc9d1\uc5d0 \ud3ec\ud568 - \ub178\uc774\uc988\uac00 \uc788\uc744 \uc218 \uc788\uc74c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post_group = zip(train_data.data, train_data.target)\n",
      "\n",
      "# Create a list of tuples that can be sorted by the length of the posts\n",
      "\n",
      "z = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]\n",
      "\n",
      "print(sorted(z)[5:7])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(90, 'Subject: E-mail of Michael Abrash?\\nFrom: gmontem@eis.calstate.edu (George A. Montemayor)\\n\\n', 'comp.graphics'), (93, 'From: passman@world.std.com (Shirley L Passman)\\nSubject: help with no docs for motherboard\\n\\n\\n', 'comp.sys.ibm.pc.hardware')]\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc704\uc758 5, 6 \ubc88\uc9f8? \uac8c\uc2dc\ubb3c \ub458\uc5d0 \ub300\ud574 \uc804\ucc98\ub9ac\ub97c \ud558\uace0 \ub098\uba74 graphics\uc5d0 \uc18d\ud55c \uac8c\uc2dc\ubb3c\uc778\uc9c0 \ud560 \uc218 \uc788\ub294 \ub2e8\uc11c\uac00 \uc5c6\ub2e4. - \ub2e4\ub978 \uac8c\uc2dc\ubb3c\uc744 \uac80\uc0c9\ud558\ub294\ub370 \ub178\uc774\uc988\uac00 \ub420 \uc218 \uc788\uc74c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer = vectorizer.build_analyzer()\n",
      "list(analyzer(z[5][1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "[u'bharper',\n",
        " u'cimlinc',\n",
        " u'uucp',\n",
        " u'brett',\n",
        " u'harper',\n",
        " u'subject',\n",
        " u'gui',\n",
        " u'applic',\n",
        " u'framework',\n",
        " u'window',\n",
        " u'hello',\n",
        " u'investig',\n",
        " u'purchas',\n",
        " u'object',\n",
        " u'orient',\n",
        " u'applic',\n",
        " u'framework',\n",
        " u'come',\n",
        " u'look',\n",
        " u'good',\n",
        " u'zapp',\n",
        " u'inmark',\n",
        " u'zinc',\n",
        " u'zinc',\n",
        " u'softwar',\n",
        " u'view',\n",
        " u'liant',\n",
        " u'win',\n",
        " u'blais',\n",
        " u'consider',\n",
        " u'use',\n",
        " u'new',\n",
        " u'window',\n",
        " u'program',\n",
        " u'unix',\n",
        " u'world',\n",
        " u'qualiti',\n",
        " u'intuitiv',\n",
        " u'abstract',\n",
        " u'class',\n",
        " u'librari',\n",
        " u'provid',\n",
        " u'import',\n",
        " u'advers',\n",
        " u'learn',\n",
        " u'intern',\n",
        " u'window',\n",
        " u'program',\n",
        " u'new',\n",
        " u'program',\n",
        " u'methodolog',\n",
        " u'close',\n",
        " u'align',\n",
        " u'nativ',\n",
        " u'don',\n",
        " u'believ',\n",
        " u'arbitrari',\n",
        " u'level',\n",
        " u'abstract',\n",
        " u'just',\n",
        " u'sake',\n",
        " u'chang',\n",
        " u'api',\n",
        " u'valuabl',\n",
        " u'develop',\n",
        " u'32bit',\n",
        " u'window',\n",
        " u'nt',\n",
        " u'memori',\n",
        " u'manag',\n",
        " u'issu',\n",
        " u'issu',\n",
        " u'particular',\n",
        " u'window',\n",
        " u'api',\n",
        " u'import',\n",
        " u'probabl',\n",
        " u'buy',\n",
        " u'class',\n",
        " u'librari',\n",
        " u'like',\n",
        " u'tool',\n",
        " u'booch',\n",
        " u'compon',\n",
        " u'ration',\n",
        " u'handl',\n",
        " u'data',\n",
        " u'structur',\n",
        " u'miscellan',\n",
        " u'stuff',\n",
        " u'alloc',\n",
        " u'featur',\n",
        " u'import',\n",
        " u'toolkit',\n",
        " u'narrow',\n",
        " u'zapp',\n",
        " u'zinc',\n",
        " u'toolkit',\n",
        " u'receiv',\n",
        " u'attent',\n",
        " u'media',\n",
        " u'wonder',\n",
        " u'hand',\n",
        " u'experi',\n",
        " u'toolkit',\n",
        " u'especi',\n",
        " u'zapp',\n",
        " u'zinc',\n",
        " u'coupl',\n",
        " u'observ',\n",
        " u'toolkit',\n",
        " u'particular',\n",
        " u'noteworthi',\n",
        " u'zapp',\n",
        " u'extens',\n",
        " u'coverag',\n",
        " u'window',\n",
        " u'function',\n",
        " u'includ',\n",
        " u'miscellan',\n",
        " u'use',\n",
        " u'class',\n",
        " u'new',\n",
        " u'fanci',\n",
        " u'3d',\n",
        " u'style',\n",
        " u'control',\n",
        " u'avail',\n",
        " u'support',\n",
        " u'custom',\n",
        " u'control',\n",
        " u'window',\n",
        " u'nt',\n",
        " u'version',\n",
        " u'essenti',\n",
        " u'redirect',\n",
        " u'graphic',\n",
        " u'display',\n",
        " u'output',\n",
        " u'architectur',\n",
        " u'use',\n",
        " u'print',\n",
        " u'sizer',\n",
        " u'class',\n",
        " u'automat',\n",
        " u'manag',\n",
        " u'control',\n",
        " u'layout',\n",
        " u'resiz',\n",
        " u'newcom',\n",
        " u'advantag',\n",
        " u'design',\n",
        " u'better',\n",
        " u'zinc',\n",
        " u'platform',\n",
        " u'independ',\n",
        " u'resourc',\n",
        " u'strategi',\n",
        " u'import',\n",
        " u'right',\n",
        " u'come',\n",
        " u'interfac',\n",
        " u'builder',\n",
        " u'tool',\n",
        " u'window',\n",
        " u'nt',\n",
        " u'version',\n",
        " u'essenti',\n",
        " u'longer',\n",
        " u'matur',\n",
        " u'grew',\n",
        " u'dos',\n",
        " u'version',\n",
        " u'better',\n",
        " u'demo',\n",
        " u'sourc',\n",
        " u'code',\n",
        " u'avail',\n",
        " u'option',\n",
        " u'lack',\n",
        " u'ole',\n",
        " u'support',\n",
        " u'particular',\n",
        " u'support',\n",
        " u'multimedia',\n",
        " u'type',\n",
        " u'stuff',\n",
        " u'thought',\n",
        " u'appreci',\n",
        " u'thank',\n",
        " u'brett',\n",
        " u'harper',\n",
        " u'brett',\n",
        " u'harper',\n",
        " u'cimlinc',\n",
        " u'com']"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ud1a0\ud070\ud654, \uc18c\ubb38\uc790\ud654, \ubd88\uc6a9\uc5b4 \uc81c\uac70 \ud6c4\uc5d0 \ub354\ud558\uc5ec min_df, max_df\ub97c \ud1b5\ud574 \uc81c\uac70\ub41c \ud6c4 \ub0a8\uc740 \ub2e8\uc5b4\ub97c \ube44\uad50\ud560 \uacbd\uc6b0 \ub178\uc774\uc988\ub294 \ub354 \uc2ec\uac01\ud574 \uc9c8 \uc218 \uc788\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(set(analyzer(z[5][1])).intersection(vectorizer.get_feature_names()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "[u'code',\n",
        " u'consider',\n",
        " u'just',\n",
        " u'lack',\n",
        " u'redirect',\n",
        " u'style',\n",
        " u'layout',\n",
        " u'miscellan',\n",
        " u'better',\n",
        " u'platform',\n",
        " u'window',\n",
        " u'program',\n",
        " u'applic',\n",
        " u'good',\n",
        " u'handl',\n",
        " u'purchas',\n",
        " u'framework',\n",
        " u'new',\n",
        " u'world',\n",
        " u'like',\n",
        " u'level',\n",
        " u'gui',\n",
        " u'arbitrari',\n",
        " u'32bit',\n",
        " u'stuff',\n",
        " u'output',\n",
        " u'resiz',\n",
        " u'manag',\n",
        " u'view',\n",
        " u'right',\n",
        " u'intern',\n",
        " u'design',\n",
        " u'close',\n",
        " u'orient',\n",
        " u'librari',\n",
        " u'wonder',\n",
        " u'multimedia',\n",
        " u'version',\n",
        " u'print',\n",
        " u'import',\n",
        " u'experi',\n",
        " u'3d',\n",
        " u'investig',\n",
        " u'appreci',\n",
        " u'extens',\n",
        " u'believ',\n",
        " u'come',\n",
        " u'thank',\n",
        " u'display',\n",
        " u'chang',\n",
        " u'narrow',\n",
        " u'com',\n",
        " u'softwar',\n",
        " u'win',\n",
        " u'independ',\n",
        " u'qualiti',\n",
        " u'automat',\n",
        " u'unix',\n",
        " u'api',\n",
        " u'fanci',\n",
        " u'avail',\n",
        " u'use',\n",
        " u'coupl',\n",
        " u'memori',\n",
        " u'support',\n",
        " u'coverag',\n",
        " u'custom',\n",
        " u'compon',\n",
        " u'interfac',\n",
        " u'includ',\n",
        " u'type',\n",
        " u'strategi',\n",
        " u'function',\n",
        " u'buy',\n",
        " u'option',\n",
        " u'especi',\n",
        " u'tool',\n",
        " u'particular',\n",
        " u'don',\n",
        " u'graphic',\n",
        " u'look',\n",
        " u'valuabl',\n",
        " u'provid',\n",
        " u'align',\n",
        " u'structur',\n",
        " u'learn',\n",
        " u'observ',\n",
        " u'control',\n",
        " u'featur',\n",
        " u'advantag',\n",
        " u'demo',\n",
        " u'abstract',\n",
        " u'toolkit',\n",
        " u'sourc',\n",
        " u'probabl',\n",
        " u'thought',\n",
        " u'uucp',\n",
        " u'develop',\n",
        " u'receiv',\n",
        " u'media',\n",
        " u'nt',\n",
        " u'architectur',\n",
        " u'matur',\n",
        " u'resourc',\n",
        " u'object',\n",
        " u'brett',\n",
        " u'hand',\n",
        " u'attent',\n",
        " u'data',\n",
        " u'class',\n",
        " u'alloc',\n",
        " u'essenti',\n",
        " u'longer',\n",
        " u'nativ',\n",
        " u'builder',\n",
        " u'issu',\n",
        " u'dos',\n",
        " u'hello']"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for term in ['cs', 'faq', 'thank', 'bh', 'thank']:\n",
      "    print('IDF(%s)=%.2f'%(term, vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "IDF(cs)=3.30\n",
        "IDF(faq)=4.25\n",
        "IDF(thank)=2.22\n",
        "IDF(bh)=6.76\n",
        "IDF(thank)=2.22\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IDF \uac12\uc774 \ub0ae\uc740 \ub2e8\uc5b4\ub4e4\uc740 \uc88b\uc740 \uac12\uc774 \uc544\ub2c8\ub2e4. \n",
      "    - \uc9c8\ubb38 \ub2f5\ubcc0 \uac8c\uc2dc\uae00\uc758 \uacbd\uc6b0 thank \uac19\uc740 \ub2e8\uc5b4\ub4e4\uc740 \uac8c\uc2dc\uae00\uc758 \uadf8\ub8f9\uacfc \uc0c1\uad00\uc5c6\uc774 \ub9ce\uc744 \uc218 \ubc16\uc5d0 \uc5c6\ub294 \ub2e8\uc5b4\uc774\ub2e4. \n",
      "    - \uc989, \uc774\ub7f0 \ub2e8\uc5b4\ub4e4\uc740 \uac8c\uc2dc\uae00\uc758 \uadf8\ub8f9\ud654\uac00 \uc798\ubabb \ub418\uc5c8\uc744 \uacbd\uc6b0 \uad00\ub828 \uac8c\uc2dc\uae00\uc744 \ucc3e\ub294\ub370 \ubc29\ud574 \uc694\uc18c\uac00 \ub420 \uc218 \uc788\ub2e4.\n",
      "    \n",
      "\uc704\uc758 \uacbd\uc6b0 IDF\uac00 6.76\uc778 bh\ub77c\ub294 \ub2e8\uc5b4\ub97c \uc81c\uc678\ud558\uace0\ub294 \uad6c\ubcc4\uc131\uc774 \uc88b\uc740 \ub2e8\uc5b4\uac00 \uc544\ub2c8\ub2e4. - \ud5a5\ud6c4 \ubcf4\uc644\uc810\n",
      "\n",
      "\uc774 \ucc45\uc758 \ubc94\uc704(\uadf8\ub8f9\ud654\uc758 \ubc94\uc704) - \uadf8\ub8f9\ud654\uac00 \uc798\ub418\uace0 \ubabb\ub418\uace0\ub294 \uc774\ud6c4\uc758 \ubb38\uc81c\uc774\uace0, \uc77c\ub2e8 \uadf8\ub8f9\ud654\ub97c \ud1b5\ud574\uc11c \ube44\uad50\ud574\uc57c\ud560 \uac8c\uc2dc\uae00\uc758 \uc218\ub97c \uc904\uc774\ub294 \ubc29\ubc95\uc744 \ud130\ub4dd"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"/Users/kimminho/Documents/Python_Study/ML_Study/BuildingMachineLearningSystemsWithPython/ch03\")\n",
      "% load noise_analysis.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import sklearn.datasets\n",
      "\n",
      "groups = [\n",
      "    'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
      "    'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
      "train_data = sklearn.datasets.fetch_20newsgroups(subset=\"train\",\n",
      "                                                 categories=groups)\n",
      "\n",
      "labels = train_data.target\n",
      "num_clusters = 50  # sp.unique(labels).shape[0]\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
      "                                    stop_words='english', decode_error='ignore'\n",
      "                                    )\n",
      "vectorized = vectorizer.fit_transform(train_data.data)\n",
      "\n",
      "post_group = zip(train_data.data, train_data.target)\n",
      "# Create a list of tuples that can be sorted by\n",
      "# the length of the posts\n",
      "all = [(len(post[0]), post[0], train_data.target_names[post[1]])\n",
      "       for post in post_group]\n",
      "graphics = sorted([post for post in all if post[2] == 'comp.graphics'])\n",
      "print(graphics[5])\n",
      "# (245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization:\n",
      "# The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\n",
      "# \\n\\n==============================================================================\\n',\n",
      "# 'comp.graphics')\n",
      "\n",
      "noise_post = graphics[5][1]\n",
      "\n",
      "analyzer = vectorizer.build_analyzer()\n",
      "print(list(analyzer(noise_post)))\n",
      "\n",
      "useful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())\n",
      "print(sorted(useful))\n",
      "# ['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n",
      "\n",
      "for term in sorted(useful):\n",
      "    print('IDF(%s)=%.2f' % (term,\n",
      "                            vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))\n",
      "# IDF(ac)=3.51\n",
      "# IDF(birmingham)=6.77\n",
      "# IDF(host)=1.74\n",
      "# IDF(kingdom)=6.68\n",
      "# IDF(nntp)=1.77\n",
      "# IDF(sorri)=4.14\n",
      "# IDF(test)=3.83\n",
      "# IDF(uk)=3.70\n",
      "# IDF(unit)=4.42\n",
      "# IDF(univers)=1.91\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(245, u'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\\n\\n==============================================================================\\n', 'comp.graphics')\n",
        "[u'situnaya', u'ibm3090', u'bham', u'ac', u'uk', u'subject', u'test', u'sorri', u'organ', u'univers', u'birmingham', u'unit', u'kingdom', u'line', u'nntp', u'post', u'host', u'ibm3090', u'bham', u'ac', u'uk']\n",
        "[u'ac', u'birmingham', u'host', u'kingdom', u'nntp', u'sorri', u'test', u'uk', u'unit', u'univers']\n",
        "IDF(ac)=3.51\n",
        "IDF(birmingham)=6.77\n",
        "IDF(host)=1.74\n",
        "IDF(kingdom)=6.68\n",
        "IDF(nntp)=1.77\n",
        "IDF(sorri)=4.14\n",
        "IDF(test)=3.83\n",
        "IDF(uk)=3.70\n",
        "IDF(unit)=4.42\n",
        "IDF(univers)=1.91\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\ub9e4\uac1c\ubcc0\uc218 \ubcc0\uacbd"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\ubcf4\ub2e4 \ub354 \uc88b\uc740 \uacb0\uacfc\ub97c \uc5bb\uae30 \uc704\ud55c \ubc29\ubc95 (\ud5a5\ud6c4 \uacfc\uc81c)\n",
      "    1. \ub2e4\ub978 \ub9e4\uac1c \ubcc0\uc218 \ubcc0\uacbd - \uad70\uc9d1\uc758 \uac1c\uc218, vectorizer\uc758 max_features \ub9e4\uac1c\ubcc0\uc218, \ub2e4\ub978 \uad70\uc9d1 \uc911\uc2ec\uc810 \ubcc0\uacbd\n",
      "    2. k\ud3c9\uade0 \uc774\uc678\uc758 \ub2e4\ub978 \uad70\uc9d1\ud654 \ubc29\ubc95 \uc0ac\uc6a9 - \ucf54\uc0ac\uc778(Cosine) \uc720\uc0ac\ub3c4, \ud53c\uc5b4\uc2a8(Pearson), \uc790\uce74\ub4dc(Jaccard) \ub4f1\ub4f1\n",
      "    \n",
      "    3. Scikit\uc5d0\ub294 Scikit.metrics\ub77c\ub294 \uad70\uc9d1(\ud654)\uc758 \uc9c8\uc744 \uce21\uc815\ud558\uae30 \uc704\ud55c \ud328\ud0a4\uc9c0\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4! - \ub354 \uc88b\uc740 \uad70\uc9d1\ud654\ub97c \uc704\ud574\uc11c\ub294 Scikit.metrics\ub97c \uacf5\ubd80\ud558\uc790!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\uc815\ub9ac"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3\uc7a5 = \uc804\ucc98\ub9ac + \uad70\uc9d1\ud654\ub97c \ud1b5\ud55c \ubb38\uc81c \ud574\uacb0\n",
      "    \n",
      "3\uc7a5 \ub0b4\uc6a9\n",
      "    - \uc804\ucc98\ub9ac \uacfc\uc815\uc758 \uc911\uc694\uc131 : \uc804\ucc98\ub9ac\uc5d0 \ub300\ud55c \ubd80\ubd84\uc774 3\uc7a5 \ub0b4\uc6a9\uc758 \uacfc\ubc18 \uc774\uc0c1 (\ucc45\uc5d0\uc11c\ub294 \uc77c\uad00\ub418\uac8c \uc88b\uc740 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc911\uc694\ud568\uc744 \uac15\uc870)\n",
      "    - \ud14d\uc2a4\ud2b8 \ucc98\ub9ac \ubc29\ubc95\n",
      "    - \ub2e8\uc5b4 \uc138\uae30\ub97c \ud1b5\ud574\uc11c \uc758\ubbf8\uc788\ub294 \uc18d\uc131(\ubca1\ud130)\ub97c \ub9cc\ub4dc\ub294 \ubc29\ubc95\n",
      "    - \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0\uc11c Scikit \ud328\ud0a4\uc9c0\uc758 \ud65c\uc6a9\n",
      "\n",
      "3\uc7a5 - \uad70\uc9d1\ud654\uc5d0 \ub300\ud574 \uc218\ubc15 \uac89\ud565\uae30\uc2dd \uc815\ub9ac : 4\uc7a5\uc774 \uc9c4\uc9dc\ub2e4!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}