{
 "metadata": {
  "name": "",
  "signature": "sha256:4ea92a9ce27f6199f4ed7c8d31dd2a7d6ff7f328cb4e3b8493c7afa8e4992e8b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os \n",
      "os.chdir(\"/home/python/notebook_root/build_machine_learning_system_with_python/data/ch03\") "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "README.md        \u001b[0m\u001b[38;5;27mdata\u001b[0m/                   rel_post_01.py      utils.py\r\n",
        "Untitled0.ipynb  noise_analysis.py       rel_post_20news.py  utils.pyc\r\n",
        "\u001b[38;5;27mcharts\u001b[0m/          plot_kmeans_example.py  tfidf.py\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load rel_post_01.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "from utils import DATA_DIR\n",
      "\n",
      "TOY_DIR = os.path.join(DATA_DIR, \"toy\")\n",
      "posts = [open(os.path.join(TOY_DIR, f)).read() for f in os.listdir(TOY_DIR)]\n",
      "\n",
      "new_post = \"imaging databases\"\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "\n",
      "class StemmedCountVectorizer(CountVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "# vectorizer = CountVectorizer(min_df=1, stop_words='english',\n",
      "# preprocessor=stemmer)\n",
      "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(\n",
      "    min_df=1, stop_words='english', decode_error='ignore')\n",
      "\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "print(new_post_vec, type(new_post_vec))\n",
      "print(new_post_vec.toarray())\n",
      "print(vectorizer.get_feature_names())\n",
      "\n",
      "\n",
      "def dist_raw(v1, v2):\n",
      "    delta = v1 - v2\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "\n",
      "    delta = v1_normalized - v2_normalized\n",
      "\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "dist = dist_norm\n",
      "\n",
      "best_dist = sys.maxsize\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "from utils import DATA_DIR\n",
      "\n",
      "TOY_DIR = os.path.join(DATA_DIR, \"toy\")\n",
      "posts = [open(os.path.join(TOY_DIR, f)).read() for f in os.listdir(TOY_DIR)]\n",
      "\n",
      "new_post = \"imaging databases\"\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "\n",
      "class StemmedCountVectorizer(CountVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "# vectorizer = CountVectorizer(min_df=1, stop_words='english',\n",
      "# preprocessor=stemmer)\n",
      "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(\n",
      "    min_df=1, stop_words='english', decode_error='ignore')\n",
      "\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "print(new_post_vec, type(new_post_vec))\n",
      "print(new_post_vec.toarray())\n",
      "print(vectorizer.get_feature_names())\n",
      "\n",
      "\n",
      "def dist_raw(v1, v2):\n",
      "    delta = v1 - v2\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "\n",
      "def dist_norm(v1, v2):\n",
      "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
      "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
      "\n",
      "    delta = v1_normalized - v2_normalized\n",
      "\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "dist = dist_norm\n",
      "\n",
      "best_dist = sys.maxsize\n",
      "best_i = None\n",
      "\n",
      "for i in range(0, num_samples):\n",
      "    post = posts[i]\n",
      "    if post == new_post:\n",
      "        continue\n",
      "    post_vec = X_train.getrow(i)\n",
      "    d = dist(post_vec, new_post_vec)\n",
      "\n",
      "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
      "\n",
      "    if d < best_dist:\n",
      "        best_dist = d\n",
      "        best_i = i\n",
      "\n",
      "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
      "print(\"source : %s\" % new_post)\n",
      "print(\"most simmrarity : %s\"%posts[best_i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 5, #features: 17\n",
        "(<1x17 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 2 stored elements in Compressed Sparse Row format>, <class 'scipy.sparse.csr.csr_matrix'>)\n",
        "[[ 0.          0.          0.          0.          0.70710678  0.70710678\n",
        "   0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.          0.        ]]\n",
        "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']\n",
        "=== Post 0 with dist=0.86: Most imaging databases save images permanently.\n",
        "\n",
        "=== Post 1 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
        "=== Post 2 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
        "=== Post 3 with dist=1.08: Imaging databases provide storage capabilities.\n",
        "=== Post 4 with dist=0.92: Imaging databases store data.\n",
        "Best post is 0 with dist=0.86\n",
        "source : imaging databases\n",
        "most simmrarity : Most imaging databases save images permanently.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load rel_post_20news.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import sklearn.datasets\n",
      "import scipy as sp\n",
      "\n",
      "new_post = \\\n",
      "    \"\"\"Disk drive problems. Hi, I have a problem with my hard disk.\n",
      "After 1 year it is working only sporadically now.\n",
      "I tried to format it, but now it doesn't boot any more.\n",
      "Any ideas? Thanks.\n",
      "\"\"\"\n",
      "\n",
      "print(\"\"\"\\\n",
      "Dear reader of the 1st edition of 'Building Machine Learning Systems with Python'!\n",
      "For the 2nd edition we introduced a couple of changes that will result into\n",
      "results that differ from the results in the 1st edition.\n",
      "E.g. we now fully rely on scikit's fetch_20newsgroups() instead of requiring\n",
      "you to download the data manually from MLCOMP.\n",
      "If you have any questions, please ask at http://www.twotoreal.com\n",
      "\"\"\")\n",
      "\n",
      "all_data = sklearn.datasets.fetch_20newsgroups(subset=\"all\")\n",
      "print(\"Number of total posts: %i\" % len(all_data.filenames))\n",
      "# Number of total posts: 18846\n",
      "\n",
      "groups = [\n",
      "    'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
      "    'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
      "train_data = sklearn.datasets.fetch_20newsgroups(subset=\"train\",\n",
      "                                                 categories=groups)\n",
      "print(\"Number of training posts in tech groups:\", len(train_data.filenames))\n",
      "# Number of training posts in tech groups: 3529\n",
      "\n",
      "labels = train_data.target\n",
      "num_clusters = 50  # sp.unique(labels).shape[0]\n",
      "\n",
      "import nltk.stem\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
      "                                    stop_words='english', decode_error='ignore'\n",
      "                                    )\n",
      "\n",
      "vectorized = vectorizer.fit_transform(train_data.data)\n",
      "num_samples, num_features = vectorized.shape\n",
      "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
      "# samples: 3529, #features: 4712\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "km = KMeans(n_clusters=num_clusters, n_init=1, verbose=1, random_state=3)\n",
      "clustered = km.fit(vectorized)\n",
      "\n",
      "print(\"km.labels_=%s\" % km.labels_)\n",
      "# km.labels_=[ 6 34 22 ...,  2 21 26]\n",
      "\n",
      "print(\"km.labels_.shape=%s\" % km.labels_.shape)\n",
      "# km.labels_.shape=3529\n",
      "\n",
      "from sklearn import metrics\n",
      "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
      "# Homogeneity: 0.400\n",
      "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
      "# Completeness: 0.206\n",
      "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
      "# V-measure: 0.272\n",
      "print(\"Adjusted Rand Index: %0.3f\" %\n",
      "      metrics.adjusted_rand_score(labels, km.labels_))\n",
      "# Adjusted Rand Index: 0.064\n",
      "print(\"Adjusted Mutual Information: %0.3f\" %\n",
      "      metrics.adjusted_mutual_info_score(labels, km.labels_))\n",
      "# Adjusted Mutual Information: 0.197\n",
      "print((\"Silhouette Coefficient: %0.3f\" %\n",
      "       metrics.silhouette_score(vectorized, labels, sample_size=1000)))\n",
      "# Silhouette Coefficient: 0.006\n",
      "\n",
      "new_post_vec = vectorizer.transform([new_post])\n",
      "new_post_label = km.predict(new_post_vec)[0]\n",
      "\n",
      "similar_indices = (km.labels_ == new_post_label).nonzero()[0]\n",
      "\n",
      "similar = []\n",
      "for i in similar_indices:\n",
      "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
      "    similar.append((dist, train_data.data[i]))\n",
      "\n",
      "similar = sorted(similar)\n",
      "print(\"Count similar: %i\" % len(similar))\n",
      "\n",
      "show_at_1 = similar[0]\n",
      "show_at_2 = similar[int(len(similar) / 10)]\n",
      "show_at_3 = similar[int(len(similar) / 2)]\n",
      "\n",
      "print(\"=== #1 ===\")\n",
      "print(show_at_1)\n",
      "print()\n",
      "\n",
      "print(\"=== #2 ===\")\n",
      "print(show_at_2)\n",
      "print()\n",
      "\n",
      "print(\"=== #3 ===\")\n",
      "print(show_at_3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dear reader of the 1st edition of 'Building Machine Learning Systems with Python'!\n",
        "For the 2nd edition we introduced a couple of changes that will result into\n",
        "results that differ from the results in the 1st edition.\n",
        "E.g. we now fully rely on scikit's fetch_20newsgroups() instead of requiring\n",
        "you to download the data manually from MLCOMP.\n",
        "If you have any questions, please ask at http://www.twotoreal.com\n",
        "\n",
        "Number of total posts: 18846"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Number of training posts in tech groups:', 3529)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "#samples: 3529, #features: 4712"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Initialization complete"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  0, inertia 5686.053\n",
        "Iteration  1, inertia 3164.888"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  2, inertia 3132.208\n",
        "Iteration  3, inertia 3111.713"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  4, inertia 3098.584\n",
        "Iteration  5, inertia 3092.191"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  6, inertia 3087.277\n",
        "Iteration  7, inertia 3084.100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  8, inertia 3082.800\n",
        "Iteration  9, inertia 3082.234"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 10, inertia 3081.949\n",
        "Iteration 11, inertia 3081.843"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 12, inertia 3081.791\n",
        "Iteration 13, inertia 3081.752"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 14, inertia 3081.660\n",
        "Iteration 15, inertia 3081.617"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 16, inertia 3081.589\n",
        "Iteration 17, inertia 3081.571"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Converged at iteration 17\n",
        "km.labels_=[48 23 31 ...,  6  2 22]\n",
        "km.labels_.shape=3529\n",
        "Homogeneity: 0.445\n",
        "Completeness: 0.231\n",
        "V-measure: 0.304\n",
        "Adjusted Rand Index: 0.094\n",
        "Adjusted Mutual Information: 0.223\n",
        "Silhouette Coefficient: 0.006"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Count similar: 56\n",
        "=== #1 ===\n",
        "(1.0378441731334072, u\"From: Thomas Dachsel <GERTHD@mvs.sas.com>\\nSubject: BOOT PROBLEM with IDE controller\\nNntp-Posting-Host: sdcmvs.mvs.sas.com\\nOrganization: SAS Institute Inc.\\nLines: 25\\n\\nHi,\\nI've got a Multi I/O card (IDE controller + serial/parallel\\ninterface) and two floppy drives (5 1/4, 3 1/2) and a\\nQuantum ProDrive 80AT connected to it.\\nI was able to format the hard disk, but I could not boot from\\nit. I can boot from drive A: (which disk drive does not matter)\\nbut if I remove the disk from drive A and press the reset switch,\\nthe LED of drive A: continues to glow, and the hard disk is\\nnot accessed at all.\\nI guess this must be a problem of either the Multi I/o card\\nor floppy disk drive settings (jumper configuration?)\\nDoes someone have any hint what could be the reason for it.\\nPlease reply by email to GERTHD@MVS.SAS.COM\\nThanks,\\nThomas\\n+-------------------------------------------------------------------+\\n| Thomas Dachsel                                                    |\\n| Internet: GERTHD@MVS.SAS.COM                                      |\\n| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\\n| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\\n| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\\n| Fax:      +49 6221 415101                                         |\\n| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\\n| Tagline:  One bad sector can ruin a whole day...                  |\\n+-------------------------------------------------------------------+\\n\")\n",
        "()\n",
        "=== #2 ===\n",
        "(1.1503043264096684, u'From: rpao@mts.mivj.ca.us (Roger C. Pao)\\nSubject: Re: Booting from B drive\\nOrganization: MicroTech Software\\nLines: 34\\n\\nglang@slee01.srl.ford.com (Gordon Lang) writes:\\n\\n>David Weisberger (djweisbe@unix.amherst.edu) wrote:\\n>: I have a 5 1/4\" drive as drive A.  How can I make the system boot from\\n>: my 3 1/2\" B drive?  (Optimally, the computer would be able to boot\\n>: from either A or B, checking them in order for a bootable disk.  But\\n>: if I have to switch cables around and simply switch the drives so that\\n>: it can\\'t boot 5 1/4\" disks, that\\'s OK.  Also, boot_b won\\'t do the trick\\n>: for me.)\\n>: \\n>: Thanks,\\n>:   Davebo\\n>We had the same issue plague us for months on our Gateway.  I finally\\n>got tired of it so I permanently interchanged the drives.  The only\\n>reason I didn\\'t do it in the first place was because I had several\\n>bootable 5-1/4\\'s and some 5-1/4 based install disks which expected\\n>the A drive.  I order all new software (and upgrades) to be 3-1/2 and\\n>the number of \"stupid\" install programs that can\\'t handle an alternate\\n>drive are declining with time - the ones I had are now upgraded.  And\\n>as for the bootable 5-1/4\\'s I just cut 3-1/2 replacements.\\n\\n>If switching the drives is not an option, you might be able to wire up\\n>a drive switch to your computer chasis.  I haven\\'t tried it but I think\\n>it would work as long as it is wired carefully.\\n\\nI did this.  I use a relay (Radio Shack 4PDT) instead of a huge\\nswitch.  This way, if the relay breaks, my drives will still work.\\n\\nIt works fine, but you may still need to change the CMOS before the\\ndrive switch will work correctly for some programs.\\n\\nrp93\\n-- \\nRoger C. Pao  {gordius,bagdad}!mts!rpao, rpao@mts.mivj.ca.us\\n')\n",
        "()\n",
        "=== #3 ===\n",
        "(1.2793959084781283, u'From: vg@volkmar.Stollmann.DE (Volkmar Grote)\\nSubject: IBM PS/1 vs TEAC FD\\nDistribution: world\\nOrganization: Me? Organized?\\nLines: 21\\n\\nHello,\\n\\nI already tried our national news group without success.\\n\\nI tried to replace a friend\\'s original IBM floppy disk in his PS/1-PC\\nwith a normal TEAC drive.\\nI already identified the power supply on pins 3 (5V) and 6 (12V), shorted\\npin 6 (5.25\"/3.5\" switch) and inserted pullup resistors (2K2) on pins\\n8, 26, 28, 30, and 34.\\nThe computer doesn\\'t complain about a missing FD, but the FD\\'s light\\nstays on all the time. The drive spins up o.k. when I insert a disk,\\nbut I can\\'t access it.\\nThe TEAC works fine in a normal PC.\\n\\nAre there any points I missed?\\n\\nThank you.\\n\\tVolkmar\\n\\n---\\nVolkmar.Grote@Stollmann.DE\\n')\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}