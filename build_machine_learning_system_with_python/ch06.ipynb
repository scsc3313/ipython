{
 "metadata": {
  "name": "",
  "signature": "sha256:6d89429c91e44b7bef1d85bd9530ee71d5ebb982c6f84b9d026a7331616772a9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os \n",
      "os.chdir(\"/backup/dev/ipython/build_machine_learning_system_with_python/data/ch06\") "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "twitter\ubb38\uc11c \uac00\uc838\uc640\uc11c \uac10\uc131 \ubd84\uc11d \ud558\uae30 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. twitter \uc218\uc9d1\uae30 \uc124\uc815 \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "dev.twitter.com/apps/new \uc811\uc18d, \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158 \uc0dd\uc131"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc5b4\ud50c\ub9ac\ucf00\uc774\uc158 \uc0dd\uc131 \ud6c4, Consumer Key, Consumer Secret, Access TOken Key, Access Token Secret \uc0dd\uc131.|\n",
      "\ud574\ub2f9 \ub0b4\uc6a9\uc740 \ub2e4\uc74c \"twitterauth.py\"\uc5d0 \ud574\ub2f9 \ub0b4\uc6a9\uc744 \uc218\uc815 "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def imgshow(filename):\n",
      "    import numpy as np\n",
      "    import matplotlib.pyplot as plt\n",
      "    import matplotlib.image as mpimg\n",
      "\n",
      "    fname = filename\n",
      "    image = mpimg.imread(fname)\n",
      "    imgplot = plt.imshow(image)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load twitterauth.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import sys\n",
      "\n",
      "CONSUMER_KEY = u'GcRkL0gdhohaKsn0FqQLWEp8y'\n",
      "CONSUMER_SECRET = u'Nbfq73KPq27u5Js0IxqQMrJz2ntuZMJmGyayz3fQhycFRhh1YV' \n",
      "\n",
      "ACCESS_TOKEN_KEY = u'50568370-23CcdsyaeI2vK5QuGlRn1IlQHjD1Aubm2yfjTTAPz'\n",
      "ACCESS_TOKEN_SECRET = u'kJ9dbaSTlVPdlK3VBtBD0sds4vLe5VjSHBydaw93LpcZ0' \n",
      "\n",
      "if CONSUMER_KEY is None or CONSUMER_SECRET is None or ACCESS_TOKEN_KEY is None or ACCESS_TOKEN_SECRET is None:\n",
      "    print(\"\"\"\\\n",
      "When doing last code sanity checks for the book, Twitter\n",
      "was using the API 1.0, which did not require authentication.\n",
      "With its switch to version 1.1, this has now changed.\n",
      "\n",
      "It seems that you don't have already created your personal Twitter\n",
      "access keys and tokens. Please do so at\n",
      "https://dev.twitter.com/docs/auth/tokens-devtwittercom\n",
      "and paste the keys/secrets into twitterauth.py\n",
      "\n",
      "Sorry for the inconvenience,\n",
      "The authors.\"\"\")\n",
      "\n",
      "    sys.exit(1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load install.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "#\n",
      "# Sanders-Twitter Sentiment Corpus Install Script\n",
      "# Version 0.1\n",
      "#\n",
      "# Pulls tweet data from Twitter because ToS prevents distributing it directly.\n",
      "#\n",
      "#   - Niek Sanders\n",
      "#     njs@sananalytics.com\n",
      "#     October 20, 2011\n",
      "#\n",
      "#\n",
      "\n",
      "# In Sanders' original form, the code was using Twitter API 1.0.\n",
      "# Now that Twitter moved to 1.1, we had to make a few changes.\n",
      "# Cf. twitterauth.py for the details.\n",
      "\n",
      "# Regarding rate limiting, please check\n",
      "# https://dev.twitter.com/rest/public/rate-limiting\n",
      "\n",
      "import sys\n",
      "import csv\n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "try:\n",
      "    import twitter\n",
      "except ImportError:\n",
      "    print(\"\"\"\\\n",
      "You need to ...\n",
      "    pip install twitter\n",
      "If pip is not found you might have to install it using easy_install.\n",
      "If it does not work on your system, you might want to follow instructions\n",
      "at https://github.com/sixohsix/twitter, most likely:\n",
      "  $ git clone https://github.com/sixohsix/twitter\n",
      "  $ cd twitter\n",
      "  $ sudo python setup.py install\n",
      "\"\"\")\n",
      "\n",
      "    sys.exit(1)\n",
      "\n",
      "from twitterauth import CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN_KEY, ACCESS_TOKEN_SECRET\n",
      "api = twitter.Twitter(auth=twitter.OAuth(consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET,\n",
      "                                         token=ACCESS_TOKEN_KEY, token_secret=ACCESS_TOKEN_SECRET))\n",
      "\n",
      "DATA_PATH = \"data\"\n",
      "\n",
      "# for some reasons TWeets disappear. In this file we collect those\n",
      "MISSING_ID_FILE = os.path.join(DATA_PATH, \"missing.tsv\")\n",
      "NOT_AUTHORIZED_ID_FILE = os.path.join(DATA_PATH, \"not_authorized.tsv\")\n",
      "\n",
      "\n",
      "def get_user_params(DATA_PATH):\n",
      "\n",
      "    user_params = {}\n",
      "\n",
      "    # get user input params\n",
      "    user_params['inList'] = os.path.join(DATA_PATH, 'corpus.csv')\n",
      "    user_params['outList'] = os.path.join(DATA_PATH, 'full-corpus.csv')\n",
      "    user_params['rawDir'] = os.path.join(DATA_PATH, 'rawdata/')\n",
      "\n",
      "    # apply defaults\n",
      "    if user_params['inList'] == '':\n",
      "        user_params['inList'] = './corpus.csv'\n",
      "    if user_params['outList'] == '':\n",
      "        user_params['outList'] = './full-corpus.csv'\n",
      "    if user_params['rawDir'] == '':\n",
      "        user_params['rawDir'] = './rawdata/'\n",
      "\n",
      "    return user_params\n",
      "\n",
      "\n",
      "def dump_user_params(user_params):\n",
      "\n",
      "    # dump user params for confirmation\n",
      "    print('Input:    ' + user_params['inList'])\n",
      "    print('Output:   ' + user_params['outList'])\n",
      "    print('Raw data: ' + user_params['rawDir'])\n",
      "\n",
      "\n",
      "def read_total_list(in_filename):\n",
      "\n",
      "    # read total fetch list csv\n",
      "    fp = open(in_filename, 'rt')\n",
      "    reader = csv.reader(fp, delimiter=',', quotechar='\"')\n",
      "\n",
      "    if os.path.exists(MISSING_ID_FILE):\n",
      "        missing_ids = [line.strip()\n",
      "                       for line in open(MISSING_ID_FILE, \"r\").readlines()]\n",
      "    else:\n",
      "        missing_ids = []\n",
      "\n",
      "    if os.path.exists(NOT_AUTHORIZED_ID_FILE):\n",
      "        not_authed_ids = [line.strip()\n",
      "                          for line in open(NOT_AUTHORIZED_ID_FILE, \"r\").readlines()]\n",
      "    else:\n",
      "        not_authed_ids = []\n",
      "\n",
      "    print(\"We will skip %i tweets that are not available or visible any more on twitter\" % (\n",
      "        len(missing_ids) + len(not_authed_ids)))\n",
      "\n",
      "    ignore_ids = set(missing_ids + not_authed_ids)\n",
      "    total_list = []\n",
      "\n",
      "    for row in reader:\n",
      "        if row[2] not in ignore_ids:\n",
      "            total_list.append(row)\n",
      "\n",
      "    return total_list\n",
      "\n",
      "\n",
      "def purge_already_fetched(fetch_list, raw_dir):\n",
      "\n",
      "    # list of tweet ids that still need downloading\n",
      "    rem_list = []\n",
      "    count_done = 0\n",
      "\n",
      "    # check each tweet to see if we have it\n",
      "    for item in fetch_list:\n",
      "\n",
      "        # check if json file exists\n",
      "        tweet_file = os.path.join(raw_dir, item[2] + '.json')\n",
      "        if os.path.exists(tweet_file):\n",
      "\n",
      "            # attempt to parse json file\n",
      "            try:\n",
      "                parse_tweet_json(tweet_file)\n",
      "                count_done += 1\n",
      "            except RuntimeError:\n",
      "                print(\"Error parsing\", item)\n",
      "                rem_list.append(item)\n",
      "        else:\n",
      "            rem_list.append(item)\n",
      "\n",
      "    print(\"We have already downloaded %i tweets.\" % count_done)\n",
      "\n",
      "    return rem_list\n",
      "\n",
      "\n",
      "def download_tweets(fetch_list, raw_dir):\n",
      "\n",
      "    # ensure raw data directory exists\n",
      "    if not os.path.exists(raw_dir):\n",
      "        os.mkdir(raw_dir)\n",
      "\n",
      "    # download tweets\n",
      "    for idx in range(0, len(fetch_list)):\n",
      "        # current item\n",
      "        item = fetch_list[idx]\n",
      "        print(item)\n",
      "\n",
      "        print('--> downloading tweet #%s (%d of %d)' %\n",
      "              (item[2], idx + 1, len(fetch_list)))\n",
      "\n",
      "        try:\n",
      "            #import pdb;pdb.set_trace()\n",
      "            response = api.statuses.show(_id=item[2])\n",
      "\n",
      "            if response.rate_limit_remaining <= 0:\n",
      "                wait_seconds = response.rate_limit_reset - time.time()\n",
      "                print(\"Rate limiting requests us to wait %f seconds\" %\n",
      "                      wait_seconds)\n",
      "                time.sleep(wait_seconds+5)\n",
      "\n",
      "        except twitter.TwitterError as e:\n",
      "            fatal = True\n",
      "            print(e)\n",
      "            for m in json.loads(e.response_data.decode())['errors']:\n",
      "                if m['code'] == 34:\n",
      "                    print(\"Tweet missing: \", item)\n",
      "                    with open(MISSING_ID_FILE, \"at\") as f:\n",
      "                        f.write(item[2] + \"\\n\")\n",
      "\n",
      "                    fatal = False\n",
      "                    break\n",
      "                elif m['code'] == 63:\n",
      "                    print(\"User of tweet '%s' has been suspended.\" % item)\n",
      "                    with open(MISSING_ID_FILE, \"at\") as f:\n",
      "                        f.write(item[2] + \"\\n\")\n",
      "\n",
      "                    fatal = False\n",
      "                    break\n",
      "                elif m['code'] == 88:\n",
      "                    print(\"Rate limit exceeded.\")\n",
      "                    fatal = True\n",
      "                    break\n",
      "                elif m['code'] == 179:\n",
      "                    print(\"Not authorized to view this tweet.\")\n",
      "                    with open(NOT_AUTHORIZED_ID_FILE, \"at\") as f:\n",
      "                        f.write(item[2] + \"\\n\")\n",
      "                    fatal = False\n",
      "                    break\n",
      "\n",
      "            if fatal:\n",
      "                raise\n",
      "            else:\n",
      "                continue\n",
      "\n",
      "        with open(raw_dir + item[2] + '.json', \"wt\") as f:\n",
      "            f.write(json.dumps(dict(response)) + \"\\n\")\n",
      "\n",
      "    return\n",
      "\n",
      "\n",
      "def parse_tweet_json(filename):\n",
      "\n",
      "    # read tweet\n",
      "    fp = open(filename, 'r')\n",
      "\n",
      "    # parse json\n",
      "    try:\n",
      "        tweet_json = json.load(fp)\n",
      "    except ValueError as e:\n",
      "        print(e)\n",
      "        raise RuntimeError('error parsing json')\n",
      "\n",
      "    # look for twitter api error msgs\n",
      "    if 'error' in tweet_json or 'errors' in tweet_json:\n",
      "        raise RuntimeError('error in downloaded tweet')\n",
      "\n",
      "    # extract creation date and tweet text\n",
      "    return [tweet_json['created_at'], tweet_json['text']]\n",
      "\n",
      "\n",
      "def build_output_corpus(out_filename, raw_dir, total_list):\n",
      "\n",
      "    # open csv output file\n",
      "    fp = open(out_filename, 'wb')\n",
      "    writer = csv.writer(fp, delimiter=',', quotechar='\"', escapechar='\\\\',\n",
      "                        quoting=csv.QUOTE_ALL)\n",
      "\n",
      "    # write header row\n",
      "    writer.writerow(\n",
      "        ['Topic', 'Sentiment', 'TweetId', 'TweetDate', 'TweetText'])\n",
      "\n",
      "    # parse all downloaded tweets\n",
      "    missing_count = 0\n",
      "    for item in total_list:\n",
      "\n",
      "        # ensure tweet exists\n",
      "        if os.path.exists(raw_dir + item[2] + '.json'):\n",
      "\n",
      "            try:\n",
      "                # parse tweet\n",
      "                parsed_tweet = parse_tweet_json(raw_dir + item[2] + '.json')\n",
      "                full_row = item + parsed_tweet\n",
      "\n",
      "                # character encoding for output\n",
      "                for i in range(0, len(full_row)):\n",
      "                    full_row[i] = full_row[i].encode(\"utf-8\")\n",
      "\n",
      "                # write csv row\n",
      "                writer.writerow(full_row)\n",
      "\n",
      "            except RuntimeError:\n",
      "                print('--> bad data in tweet #' + item[2])\n",
      "                missing_count += 1\n",
      "\n",
      "        else:\n",
      "            print('--> missing tweet #' + item[2])\n",
      "            missing_count += 1\n",
      "\n",
      "    # indicate success\n",
      "    if missing_count == 0:\n",
      "        print('\\nSuccessfully downloaded corpus!')\n",
      "        print('Output in: ' + out_filename + '\\n')\n",
      "    else:\n",
      "        print('\\nMissing %d of %d tweets!' % (missing_count, len(total_list)))\n",
      "        print('Partial output in: ' + out_filename + '\\n')\n",
      "\n",
      "    return\n",
      "\n",
      "\n",
      "def main():\n",
      "    # get user parameters\n",
      "    user_params = get_user_params(DATA_PATH)\n",
      "    print(user_params)\n",
      "    dump_user_params(user_params)\n",
      "\n",
      "    # get fetch list\n",
      "    total_list = read_total_list(user_params['inList'])\n",
      "\n",
      "    # remove already fetched or missing tweets\n",
      "    fetch_list = purge_already_fetched(total_list, user_params['rawDir'])\n",
      "    print(\"Fetching %i tweets...\" % len(fetch_list))\n",
      "\n",
      "    if fetch_list:\n",
      "        # start fetching data from twitter\n",
      "        download_tweets(fetch_list, user_params['rawDir'])\n",
      "\n",
      "        # second pass for any failed downloads\n",
      "        fetch_list = purge_already_fetched(total_list, user_params['rawDir'])\n",
      "        if fetch_list:\n",
      "            print('\\nStarting second pass to retry %i failed downloads...' %\n",
      "                  len(fetch_list))\n",
      "            download_tweets(fetch_list, user_params['rawDir'])\n",
      "    else:\n",
      "        print(\"Nothing to fetch any more.\")\n",
      "\n",
      "    # build output corpus\n",
      "    build_output_corpus(user_params['outList'], user_params['rawDir'],\n",
      "                        total_list)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'rawDir': 'data/rawdata/', 'inList': 'data/corpus.csv', 'outList': 'data/full-corpus.csv'}\n",
        "Input:    data/corpus.csv\n",
        "Output:   data/full-corpus.csv\n",
        "Raw data: data/rawdata/\n",
        "We will skip 1165 tweets that are not available or visible any more on twitter\n",
        "We have already downloaded 1046 tweets."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching 3680 tweets...\n",
        "['google', 'positive', '126522262768726016']\n",
        "--> downloading tweet #126522262768726016 (1 of 3680)\n",
        "['google', 'positive', '126521286053724160']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126521286053724160 (2 of 3680)\n",
        "['google', 'positive', '126520029410885632']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126520029410885632 (3 of 3680)\n",
        "['google', 'positive', '126519483752914944']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126519483752914944 (4 of 3680)\n",
        "['google', 'positive', '126519329025040384']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126519329025040384 (5 of 3680)\n",
        "['google', 'positive', '126519123772588032']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126519123772588032 (6 of 3680)\n",
        "['google', 'positive', '126519017405030400']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126519017405030400 (7 of 3680)\n",
        "Twitter sent status 404 for URL: 1.1/statuses/show.json using parameters: (id=126519017405030400&oauth_consumer_key=GcRkL0gdhohaKsn0FqQLWEp8y&oauth_nonce=7575555518579064322&oauth_signature_method=HMAC-SHA1&oauth_timestamp=1422447301&oauth_token=50568370-23CcdsyaeI2vK5QuGlRn1IlQHjD1Aubm2yfjTTAPz&oauth_version=1.0&oauth_signature=rp1zBiI3c8l3QTxMqAK0doHx1SA%3D)\n",
        "details: {\"errors\":[{\"message\":\"Sorry, that page does not exist\",\"code\":34}]}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Tweet missing: ', ['google', 'positive', '126519017405030400'])\n",
        "['google', 'positive', '126518882939838464']\n",
        "--> downloading tweet #126518882939838464 (8 of 3680)\n",
        "['google', 'positive', '126516779886456832']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126516779886456832 (9 of 3680)\n",
        "['google', 'positive', '126516304336257025']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126516304336257025 (10 of 3680)\n",
        "Twitter sent status 404 for URL: 1.1/statuses/show.json using parameters: (id=126516304336257025&oauth_consumer_key=GcRkL0gdhohaKsn0FqQLWEp8y&oauth_nonce=11115837532752525744&oauth_signature_method=HMAC-SHA1&oauth_timestamp=1422447306&oauth_token=50568370-23CcdsyaeI2vK5QuGlRn1IlQHjD1Aubm2yfjTTAPz&oauth_version=1.0&oauth_signature=%2BYEFWPPNS5yzx7G4l8xDpAtnzYk%3D)\n",
        "details: {\"errors\":[{\"message\":\"Sorry, that page does not exist\",\"code\":34}]}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Tweet missing: ', ['google', 'positive', '126516304336257025'])\n",
        "['google', 'positive', '126515760855134208']\n",
        "--> downloading tweet #126515760855134208 (11 of 3680)\n",
        "['google', 'positive', '126514474378203136']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126514474378203136 (12 of 3680)\n",
        "Twitter sent status 403 for URL: 1.1/statuses/show.json using parameters: (id=126514474378203136&oauth_consumer_key=GcRkL0gdhohaKsn0FqQLWEp8y&oauth_nonce=2376696123320822758&oauth_signature_method=HMAC-SHA1&oauth_timestamp=1422447309&oauth_token=50568370-23CcdsyaeI2vK5QuGlRn1IlQHjD1Aubm2yfjTTAPz&oauth_version=1.0&oauth_signature=M1OI3%2FQ5bo%2F3beU2svBTO8N0eCI%3D)\n",
        "details: {\"errors\":[{\"code\":179,\"message\":\"Sorry, you are not authorized to see this status.\"}]}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Not authorized to view this tweet.\n",
        "['google', 'positive', '126513620686352384']\n",
        "--> downloading tweet #126513620686352384 (13 of 3680)\n",
        "['google', 'positive', '126513526385819648']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126513526385819648 (14 of 3680)\n",
        "['google', 'positive', '126513425043030016']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126513425043030016 (15 of 3680)\n",
        "['google', 'positive', '126512728297844736']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126512728297844736 (16 of 3680)\n",
        "['google', 'positive', '126512631937904640']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126512631937904640 (17 of 3680)\n",
        "['google', 'positive', '126512208451600384']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126512208451600384 (18 of 3680)\n",
        "['google', 'positive', '126511545160171520']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126511545160171520 (19 of 3680)\n",
        "['google', 'positive', '126511426926944256']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126511426926944256 (20 of 3680)\n",
        "['google', 'positive', '126511000907288576']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126511000907288576 (21 of 3680)\n",
        "['google', 'positive', '126510977335300096']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126510977335300096 (22 of 3680)\n",
        "['google', 'positive', '126510551789604864']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126510551789604864 (23 of 3680)\n",
        "['google', 'positive', '126509929619132417']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126509929619132417 (24 of 3680)\n",
        "['google', 'positive', '126509528287166464']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126509528287166464 (25 of 3680)\n",
        "['google', 'positive', '126508433582211072']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126508433582211072 (26 of 3680)\n",
        "['google', 'positive', '126508393203642368']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126508393203642368 (27 of 3680)\n",
        "['google', 'positive', '126507456019968000']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126507456019968000 (28 of 3680)\n",
        "['google', 'positive', '126507105023819776']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126507105023819776 (29 of 3680)\n",
        "['google', 'positive', '126506850781888512']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126506850781888512 (30 of 3680)\n",
        "['google', 'positive', '126506410195431424']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126506410195431424 (31 of 3680)\n",
        "['google', 'positive', '126506064387637249']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126506064387637249 (32 of 3680)\n",
        "['google', 'positive', '126505384428052481']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126505384428052481 (33 of 3680)\n",
        "['google', 'positive', '126505187752943616']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126505187752943616 (34 of 3680)\n",
        "['google', 'positive', '126505144878772224']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126505144878772224 (35 of 3680)\n",
        "['google', 'positive', '126504782465732608']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126504782465732608 (36 of 3680)\n",
        "['google', 'positive', '126504452680187905']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126504452680187905 (37 of 3680)\n",
        "['google', 'positive', '126504346639802368']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126504346639802368 (38 of 3680)\n",
        "['google', 'positive', '126504216004005888']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126504216004005888 (39 of 3680)\n",
        "['google', 'positive', '126504013939216384']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126504013939216384 (40 of 3680)\n",
        "['google', 'positive', '126503946092158976']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126503946092158976 (41 of 3680)\n",
        "['google', 'positive', '126503805369069568']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126503805369069568 (42 of 3680)\n",
        "['google', 'positive', '126503790412181504']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126503790412181504 (43 of 3680)\n",
        "['google', 'positive', '126503029548654593']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126503029548654593 (44 of 3680)\n",
        "['google', 'positive', '126502415322193920']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126502415322193920 (45 of 3680)\n",
        "['google', 'positive', '126502014560649216']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126502014560649216 (46 of 3680)\n",
        "['google', 'positive', '126501468902658048']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126501468902658048 (47 of 3680)\n",
        "['google', 'positive', '126500614552289282']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126500614552289282 (48 of 3680)\n",
        "['google', 'positive', '126499965869625345']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126499965869625345 (49 of 3680)\n",
        "Twitter sent status 403 for URL: 1.1/statuses/show.json using parameters: (id=126499965869625345&oauth_consumer_key=GcRkL0gdhohaKsn0FqQLWEp8y&oauth_nonce=9613619980732726130&oauth_signature_method=HMAC-SHA1&oauth_timestamp=1422447366&oauth_token=50568370-23CcdsyaeI2vK5QuGlRn1IlQHjD1Aubm2yfjTTAPz&oauth_version=1.0&oauth_signature=GhZkaGpM0ZFX7oCRvT%2FgqbR6E1A%3D)\n",
        "details: {\"errors\":[{\"code\":179,\"message\":\"Sorry, you are not authorized to see this status.\"}]}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Not authorized to view this tweet.\n",
        "['google', 'positive', '126499428965158912']\n",
        "--> downloading tweet #126499428965158912 (50 of 3680)\n",
        "['google', 'positive', '126499145014980608']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126499145014980608 (51 of 3680)\n",
        "['google', 'positive', '126499143282737152']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126499143282737152 (52 of 3680)\n",
        "['google', 'positive', '126498734409396224']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126498734409396224 (53 of 3680)\n",
        "['google', 'positive', '126498608815149056']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126498608815149056 (54 of 3680)\n",
        "['google', 'positive', '126498587499696128']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126498587499696128 (55 of 3680)\n",
        "['google', 'positive', '126497976314109952']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126497976314109952 (56 of 3680)\n",
        "['google', 'positive', '126497860752646146']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126497860752646146 (57 of 3680)\n",
        "['google', 'positive', '126497655785402368']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126497655785402368 (58 of 3680)\n",
        "['google', 'positive', '126496772586610688']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126496772586610688 (59 of 3680)\n",
        "['google', 'positive', '126496739531304960']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126496739531304960 (60 of 3680)\n",
        "['google', 'positive', '126496342901133313']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126496342901133313 (61 of 3680)\n",
        "['google', 'positive', '126496262668292096']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126496262668292096 (62 of 3680)\n",
        "['google', 'positive', '126496155856142336']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126496155856142336 (63 of 3680)\n",
        "['google', 'positive', '126496005809127424']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126496005809127424 (64 of 3680)\n",
        "['google', 'positive', '126495843116265475']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126495843116265475 (65 of 3680)\n",
        "['google', 'positive', '126495812724338688']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126495812724338688 (66 of 3680)\n",
        "['google', 'positive', '126495283176685569']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126495283176685569 (67 of 3680)\n",
        "['google', 'positive', '126494976396898305']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494976396898305 (68 of 3680)\n",
        "['google', 'positive', '126494883367235585']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494883367235585 (69 of 3680)\n",
        "['google', 'positive', '126494838689513473']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494838689513473 (70 of 3680)\n",
        "['google', 'positive', '126494645281755136']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494645281755136 (71 of 3680)\n",
        "['google', 'positive', '126494573966016512']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494573966016512 (72 of 3680)\n",
        "['google', 'positive', '126494442290020352']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494442290020352 (73 of 3680)\n",
        "['google', 'positive', '126494339248562176']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494339248562176 (74 of 3680)\n",
        "['google', 'positive', '126494221879357440']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494221879357440 (75 of 3680)\n",
        "['google', 'positive', '126494070385283072']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126494070385283072 (76 of 3680)\n",
        "['google', 'positive', '126493889761787904']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126493889761787904 (77 of 3680)\n",
        "['google', 'positive', '126493850914131968']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126493850914131968 (78 of 3680)\n",
        "['google', 'positive', '126493722916560896']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126493722916560896 (79 of 3680)\n",
        "['google', 'positive', '126493648757071873']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--> downloading tweet #126493648757071873 (80 of 3680)\n",
        "['google', 'positive', '126493639605092352']"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load utils.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import collections\n",
      "import csv\n",
      "import json\n",
      "\n",
      "from matplotlib import pylab\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "DATA_DIR = \"data\"\n",
      "CHART_DIR = \"charts\"\n",
      "\n",
      "if not os.path.exists(DATA_DIR):\n",
      "    raise RuntimeError(\"Expecting directory 'data' in current path\")\n",
      "\n",
      "if not os.path.exists(CHART_DIR):\n",
      "    os.mkdir(CHART_DIR)\n",
      "\n",
      "\n",
      "def tweak_labels(Y, pos_sent_list):\n",
      "    pos = Y == pos_sent_list[0]\n",
      "    for sent_label in pos_sent_list[1:]:\n",
      "        pos |= Y == sent_label\n",
      "\n",
      "    Y = np.zeros(Y.shape[0])\n",
      "    Y[pos] = 1\n",
      "    Y = Y.astype(int)\n",
      "\n",
      "    return Y\n",
      "\n",
      "\n",
      "def load_sanders_data(dirname=\".\", line_count=-1):\n",
      "    count = 0\n",
      "\n",
      "    topics = []\n",
      "    labels = []\n",
      "    tweets = []\n",
      "\n",
      "    with open(os.path.join(DATA_DIR, dirname, \"corpus.csv\"), \"r\") as csvfile:\n",
      "        metareader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
      "        for line in metareader:\n",
      "            count += 1\n",
      "            if line_count > 0 and count > line_count:\n",
      "                break\n",
      "\n",
      "            topic, label, tweet_id = line\n",
      "\n",
      "            tweet_fn = os.path.join(\n",
      "                DATA_DIR, dirname, 'rawdata', '%s.json' % tweet_id)\n",
      "            try:\n",
      "                tweet = json.load(open(tweet_fn, \"r\"))\n",
      "            except IOError:\n",
      "                print((\"Tweet '%s' not found. Skip.\" % tweet_fn))\n",
      "                continue\n",
      "\n",
      "            if 'text' in tweet and tweet['user']['lang'] == \"en\":\n",
      "                topics.append(topic)\n",
      "                labels.append(label)\n",
      "                tweets.append(tweet['text'])\n",
      "\n",
      "    tweets = np.asarray(tweets)\n",
      "    labels = np.asarray(labels)\n",
      "\n",
      "    return tweets, labels\n",
      "\n",
      "\n",
      "def plot_pr(auc_score, name, phase, precision, recall, label=None):\n",
      "    pylab.clf()\n",
      "    pylab.figure(num=None, figsize=(5, 4))\n",
      "    pylab.grid(True)\n",
      "    pylab.fill_between(recall, precision, alpha=0.5)\n",
      "    pylab.plot(recall, precision, lw=1)\n",
      "    pylab.xlim([0.0, 1.0])\n",
      "    pylab.ylim([0.0, 1.0])\n",
      "    pylab.xlabel('Recall')\n",
      "    pylab.ylabel('Precision')\n",
      "    pylab.title('P/R curve (AUC=%0.2f) / %s' % (auc_score, label))\n",
      "    filename = name.replace(\" \", \"_\")\n",
      "    pylab.savefig(os.path.join(CHART_DIR, \"pr_%s_%s.png\" %\n",
      "                               (filename, phase)), bbox_inches=\"tight\")\n",
      "\n",
      "\n",
      "def show_most_informative_features(vectorizer, clf, n=20):\n",
      "    c_f = sorted(zip(clf.coef_[0], vectorizer.get_feature_names()))\n",
      "    top = list(zip(c_f[:n], c_f[:-(n + 1):-1]))\n",
      "    for (c1, f1), (c2, f2) in top:\n",
      "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (c1, f1, c2, f2))\n",
      "\n",
      "\n",
      "def plot_log():\n",
      "    pylab.clf()\n",
      "    pylab.figure(num=None, figsize=(6, 5))\n",
      "\n",
      "    x = np.arange(0.001, 1, 0.001)\n",
      "    y = np.log(x)\n",
      "\n",
      "    pylab.title('Relationship between probabilities and their logarithm')\n",
      "    pylab.plot(x, y)\n",
      "    pylab.grid(True)\n",
      "    pylab.xlabel('P')\n",
      "    pylab.ylabel('log(P)')\n",
      "    filename = 'log_probs.png'\n",
      "    pylab.savefig(os.path.join(CHART_DIR, filename), bbox_inches=\"tight\")\n",
      "\n",
      "\n",
      "def plot_feat_importance(feature_names, clf, name):\n",
      "    pylab.clf()\n",
      "    coef_ = clf.coef_\n",
      "    important = np.argsort(np.absolute(coef_.ravel()))\n",
      "    f_imp = feature_names[important]\n",
      "    coef = coef_.ravel()[important]\n",
      "    inds = np.argsort(coef)\n",
      "    f_imp = f_imp[inds]\n",
      "    coef = coef[inds]\n",
      "    xpos = np.array(list(range(len(coef))))\n",
      "    pylab.bar(xpos, coef, width=1)\n",
      "\n",
      "    pylab.title('Feature importance for %s' % (name))\n",
      "    ax = pylab.gca()\n",
      "    ax.set_xticks(np.arange(len(coef)))\n",
      "    labels = ax.set_xticklabels(f_imp)\n",
      "    for label in labels:\n",
      "        label.set_rotation(90)\n",
      "    filename = name.replace(\" \", \"_\")\n",
      "    pylab.savefig(os.path.join(\n",
      "        CHART_DIR, \"feat_imp_%s.png\" % filename), bbox_inches=\"tight\")\n",
      "\n",
      "\n",
      "def plot_feat_hist(data_name_list, filename=None):\n",
      "    pylab.clf()\n",
      "    num_rows = 1 + (len(data_name_list) - 1) / 2\n",
      "    num_cols = 1 if len(data_name_list) == 1 else 2\n",
      "    pylab.figure(figsize=(5 * num_cols, 4 * num_rows))\n",
      "\n",
      "    for i in range(num_rows):\n",
      "        for j in range(num_cols):\n",
      "            pylab.subplot(num_rows, num_cols, 1 + i * num_cols + j)\n",
      "            x, name = data_name_list[i * num_cols + j]\n",
      "            pylab.title(name)\n",
      "            pylab.xlabel('Value')\n",
      "            pylab.ylabel('Density')\n",
      "            # the histogram of the data\n",
      "            max_val = np.max(x)\n",
      "            if max_val <= 1.0:\n",
      "                bins = 50\n",
      "            elif max_val > 50:\n",
      "                bins = 50\n",
      "            else:\n",
      "                bins = max_val\n",
      "            n, bins, patches = pylab.hist(\n",
      "                x, bins=bins, normed=1, facecolor='green', alpha=0.75)\n",
      "\n",
      "            pylab.grid(True)\n",
      "\n",
      "    if not filename:\n",
      "        filename = \"feat_hist_%s.png\" % name\n",
      "\n",
      "    pylab.savefig(os.path.join(CHART_DIR, filename), bbox_inches=\"tight\")\n",
      "\n",
      "\n",
      "def plot_bias_variance(data_sizes, train_errors, test_errors, name):\n",
      "    pylab.clf()\n",
      "    pylab.ylim([0.0, 1.0])\n",
      "    pylab.xlabel('Data set size')\n",
      "    pylab.ylabel('Error')\n",
      "    pylab.title(\"Bias-Variance for '%s'\" % name)\n",
      "    pylab.plot(\n",
      "        data_sizes, train_errors, \"-\", data_sizes, test_errors, \"--\", lw=1)\n",
      "    pylab.legend([\"train error\", \"test error\"], loc=\"upper right\")\n",
      "    pylab.grid()\n",
      "    pylab.savefig(os.path.join(CHART_DIR, \"bv_\" + name + \".png\"))\n",
      "\n",
      "\n",
      "def load_sent_word_net():\n",
      "\n",
      "    sent_scores = collections.defaultdict(list)\n",
      "    sentiwordnet_path = os.path.join(DATA_DIR, \"SentiWordNet_3.0.0_20130122.txt\")\n",
      "\n",
      "    if not os.path.exists(sentiwordnet_path):\n",
      "        print(\"Please download SentiWordNet_3.0.0 from http://sentiwordnet.isti.cnr.it/download.php, extract it and put it into the data directory\")\n",
      "        sys.exit(1)\n",
      "\n",
      "    with open(sentiwordnet_path, 'r') as csvfile:\n",
      "        reader = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
      "        for line in reader:\n",
      "            if line[0].startswith(\"#\"):\n",
      "                continue\n",
      "            if len(line) == 1:\n",
      "                continue\n",
      "\n",
      "            POS, ID, PosScore, NegScore, SynsetTerms, Gloss = line\n",
      "            if len(POS) == 0 or len(ID) == 0:\n",
      "                continue\n",
      "            # print POS,PosScore,NegScore,SynsetTerms\n",
      "            for term in SynsetTerms.split(\" \"):\n",
      "                # drop #number at the end of every term\n",
      "                term = term.split(\"#\")[0]\n",
      "                term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n",
      "                key = \"%s/%s\" % (POS, term.split(\"#\")[0])\n",
      "                sent_scores[key].append((float(PosScore), float(NegScore)))\n",
      "    for key, value in sent_scores.items():\n",
      "        sent_scores[key] = np.mean(value, axis=0)\n",
      "\n",
      "    return sent_scores\n",
      "\n",
      "\n",
      "def log_false_positives(clf, X, y, name):\n",
      "    with open(\"FP_\" + name.replace(\" \", \"_\") + \".tsv\", \"w\") as f:\n",
      "        false_positive = clf.predict(X) != y\n",
      "        for tweet, false_class in zip(X[false_positive], y[false_positive]):\n",
      "            f.write(\"%s\\t%s\\n\" %\n",
      "                    (false_class, tweet.encode(\"ascii\", \"ignore\")))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    plot_log()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load 01_start.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code is supporting material for the book\n",
      "# Building Machine Learning Systems with Python\n",
      "# by Willi Richert and Luis Pedro Coelho\n",
      "# published by PACKT Publishing\n",
      "#\n",
      "# It is made available under the MIT License\n",
      "\n",
      "#\n",
      "# This script trains multinomial Naive Bayes on the tweet corpus\n",
      "# to find two different results:\n",
      "# - How well can we distinguis positive from negative tweets?\n",
      "# - How well can we detect whether a tweet contains sentiment at all?\n",
      "#\n",
      "\n",
      "import time\n",
      "start_time = time.time()\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "\n",
      "from utils import plot_pr\n",
      "from utils import load_sanders_data\n",
      "from utils import tweak_labels\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "\n",
      "def create_ngram_model():\n",
      "    tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3),\n",
      "                                   analyzer=\"word\", binary=False)\n",
      "    clf = MultinomialNB()\n",
      "    pipeline = Pipeline([('vect', tfidf_ngrams), ('clf', clf)])\n",
      "    return pipeline\n",
      "\n",
      "\n",
      "def train_model(clf_factory, X, Y, name=\"NB ngram\", plot=False):\n",
      "    cv = ShuffleSplit(\n",
      "        n=len(X), n_iter=10, test_size=0.3, random_state=0)\n",
      "\n",
      "    train_errors = []\n",
      "    test_errors = []\n",
      "\n",
      "    scores = []\n",
      "    pr_scores = []\n",
      "    precisions, recalls, thresholds = [], [], []\n",
      "\n",
      "    for train, test in cv:\n",
      "        X_train, y_train = X[train], Y[train]\n",
      "        X_test, y_test = X[test], Y[test]\n",
      "\n",
      "        clf = clf_factory()\n",
      "        clf.fit(X_train, y_train)\n",
      "\n",
      "        train_score = clf.score(X_train, y_train)\n",
      "        test_score = clf.score(X_test, y_test)\n",
      "\n",
      "        train_errors.append(1 - train_score)\n",
      "        test_errors.append(1 - test_score)\n",
      "\n",
      "        scores.append(test_score)\n",
      "        proba = clf.predict_proba(X_test)\n",
      "\n",
      "        fpr, tpr, roc_thresholds = roc_curve(y_test, proba[:, 1])\n",
      "        precision, recall, pr_thresholds = precision_recall_curve(\n",
      "            y_test, proba[:, 1])\n",
      "\n",
      "        pr_scores.append(auc(recall, precision))\n",
      "        precisions.append(precision)\n",
      "        recalls.append(recall)\n",
      "        thresholds.append(pr_thresholds)\n",
      "\n",
      "    scores_to_sort = pr_scores\n",
      "    median = np.argsort(scores_to_sort)[len(scores_to_sort) / 2]\n",
      "\n",
      "    if plot:\n",
      "        plot_pr(pr_scores[median], name, \"01\", precisions[median],\n",
      "                recalls[median], label=name)\n",
      "\n",
      "        summary = (np.mean(scores), np.std(scores),\n",
      "                   np.mean(pr_scores), np.std(pr_scores))\n",
      "        print(\"%.3f\\t%.3f\\t%.3f\\t%.3f\\t\" % summary)\n",
      "\n",
      "    return np.mean(train_errors), np.mean(test_errors)\n",
      "\n",
      "\n",
      "def print_incorrect(clf, X, Y):\n",
      "    Y_hat = clf.predict(X)\n",
      "    wrong_idx = Y_hat != Y\n",
      "    X_wrong = X[wrong_idx]\n",
      "    Y_wrong = Y[wrong_idx]\n",
      "    Y_hat_wrong = Y_hat[wrong_idx]\n",
      "    for idx in range(len(X_wrong)):\n",
      "        print(\"clf.predict('%s')=%i instead of %i\" %\n",
      "              (X_wrong[idx], Y_hat_wrong[idx], Y_wrong[idx]))\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    X_orig, Y_orig = load_sanders_data()\n",
      "    classes = np.unique(Y_orig)\n",
      "    for c in classes:\n",
      "        print(\"#%s: %i\" % (c, sum(Y_orig == c)))\n",
      "\n",
      "    print(\"== Pos vs. neg ==\")\n",
      "    pos_neg = np.logical_or(Y_orig == \"positive\", Y_orig == \"negative\")\n",
      "    X = X_orig[pos_neg]\n",
      "    Y = Y_orig[pos_neg]\n",
      "    Y = tweak_labels(Y, [\"positive\"])\n",
      "\n",
      "    train_model(create_ngram_model, X, Y, name=\"pos vs neg\", plot=True)\n",
      "\n",
      "    print(\"== Pos/neg vs. irrelevant/neutral ==\")\n",
      "    X = X_orig\n",
      "    Y = tweak_labels(Y_orig, [\"positive\", \"negative\"])\n",
      "    train_model(create_ngram_model, X, Y, name=\"sent vs rest\", plot=True)\n",
      "\n",
      "    print(\"== Pos vs. rest ==\")\n",
      "    X = X_orig\n",
      "    Y = tweak_labels(Y_orig, [\"positive\"])\n",
      "    train_model(create_ngram_model, X, Y, name=\"pos vs rest\", plot=True)\n",
      "\n",
      "    print(\"== Neg vs. rest ==\")\n",
      "    X = X_orig\n",
      "    Y = tweak_labels(Y_orig, [\"negative\"])\n",
      "    train_model(create_ngram_model, X, Y, name=\"neg vs rest\", plot=True)\n",
      "\n",
      "    print(\"time spent:\", time.time() - start_time)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tweet 'data/./rawdata/126394830791254016.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126366123368267776.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126354605130002432.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126350948548354048.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126350302113824769.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126344048637259776.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126323785145126912.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126319186141130752.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126312877916307458.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126267185025916928.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126213333123743744.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126148685737361408.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126130991365500928.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126093298619252737.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126079414986485761.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126076743613284354.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126063569660936193.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126063358037340161.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126059399319003136.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126049183865114624.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126040352237961217.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126039929523404801.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126015087386431488.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126008369562652672.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125995158679461888.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125979338846900224.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125978473712979969.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125954443643588608.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125947232359948288.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125943290288803841.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125932869389524992.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125930171562852353.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125910633731461120.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125909565031198720.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125907732388790272.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125898611572740097.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125816853867151360.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125727629012770816.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125688922410975232.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125685656415510528.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125673004511412224.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125665606853861376.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125633065757310976.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125616280215617537.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125595292304281601.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125536884813336576.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125533599737978882.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125458901192810496.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125445056218923008.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125422502284505088.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125402636764712960.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125330595302744064.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125281706327552001.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125279447669669888.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125264731035537409.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125251672896323584.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125243911538098176.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126400637930979329.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126391082308206593.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126328782700285952.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126325125749542913.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126312535203921920.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126311879218966529.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126308005779210241.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126304942049853441.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126297241190281216.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126295434862936064.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126282994821509120.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126270073420791810.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126264313563459585.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126239832895795200.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126182880123695104.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126156590662422528.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126153311521996800.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126148955217203200.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126148565302128640.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126140389873827841.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126129938247061504.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126126605344047105.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126121175926571009.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126082198720888833.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126073788323479552.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126070647125327872.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126060639268507649.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126042137740574720.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126042022900547584.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126033747991736320.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126012404332113920.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126012089415380992.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126006088725303296.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/126004661248471040.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125988395787882497.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125974810021998595.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125969502587465729.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125954651152592896.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125934808592433153.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125836461936361472.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125824054958637056.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125814380871946240.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125812985301172224.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125811345064067072.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125799384976863232.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125794882257305600.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125728250579259392.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125722610179907584.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125711996074209280.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125694815743651840.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125692685033021441.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125692532750430209.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125681375058735104.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125675806977556480.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125673358418391041.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125659125886623744.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125656618326175745.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125619303356710912.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125607492356018176.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125588697872728065.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125578269197217792.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125561930416013312.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125547255947198465.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125476730067615744.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125475953509015552.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125441732941840385.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125435218017525760.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125420263687995392.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125400161886277632.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125395636219678720.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125380163302199296.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125371779039502336.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125369698840887297.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125368089159286784.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125355139409252352.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125347619072512000.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125346783390990337.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125334519254482944.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125330337847975937.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125309427422203904.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125304159581900800.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125295729139908608.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125294978623746048.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125271422431014914.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125268117680160768.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125245246136258561.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125232266849947648.json' not found. Skip.\n",
        "Tweet 'data/./rawdata/125212404299735040.json' not found. Skip.\n",
       